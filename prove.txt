

# FILE ONLY FOR TESTING PURPOSE



# visualize the prediction 
python entrypoint_predict.py --name nonso --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar --folder datasets/examples/
python entrypoint_predict.py --options /path/to/yml --checkpoint /path/to/checkpoint --folder /path/to/images



#training all 
python entrypoint_train.py --name nonso --options experiments/default/resnet.yml

#training new model (for fine tunning)
python entrypoint_train_ga.py --name nonso --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar --folder datasets/examples/

#new training new model (beta test)
python entrypoint_beta.py --name nonsonew --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar 


#test all

python entrypoint_eval.py --name prova_0 --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar 

python entrypoint_train_ga.py --name prova_1 --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar 


''' RESULT ORIGINAL (RESNET) '''
Test Step 000000/005472 (000000) cd 0.000697 (0.000054), f1_tau 0.391795 (0.030138), f1_2tau 0.590053 (0.045389)
Test Step 000050/005472 (000050) cd 0.000916 (0.000070), f1_tau 0.319681 (0.024591), f1_2tau 0.505144 (0.038857)
Test Step 000100/005472 (000100) cd 0.000895 (0.000069), f1_tau 0.321660 (0.024743), f1_2tau 0.507799 (0.039061)
Test Step 000150/005472 (000150) cd 0.000916 (0.000070), f1_tau 0.320202 (0.024631), f1_2tau 0.503963 (0.038766)
Test Step 000200/005472 (000200) cd 0.000924 (0.000071), f1_tau 0.322334 (0.024795), f1_2tau 0.507239 (0.039018)
Test Step 000250/005472 (000250) cd 0.000931 (0.000072), f1_tau 0.321201 (0.024708), f1_2tau 0.505700 (0.038900)
Test Step 000300/005472 (000300) cd 0.000921 (0.000071), f1_tau 0.321062 (0.024697), f1_2tau 0.505326 (0.038871)
Test Step 000350/005472 (000350) cd 0.000915 (0.000070), f1_tau 0.320822 (0.024679), f1_2tau 0.505116 (0.038855)
Test Step 000400/005472 (000400) cd 0.000911 (0.000127), f1_tau 0.322159 (0.050435), f1_2tau 0.506425 (0.079279)
Test Step 000450/005472 (000450) cd 0.000893 (0.000128), f1_tau 0.322063 (0.049563), f1_2tau 0.508164 (0.079117)
Test Step 000500/005472 (000500) cd 0.000878 (0.000128), f1_tau 0.321384 (0.049304), f1_2tau 0.508894 (0.078866)
Test Step 000550/005472 (000550) cd 0.000864 (0.000127), f1_tau 0.322225 (0.049596), f1_2tau 0.511139 (0.079230)
Test Step 000600/005472 (000600) cd 0.000857 (0.000183), f1_tau 0.322832 (0.083742), f1_2tau 0.512013 (0.125279)
Test Step 000650/005472 (000650) cd 0.000850 (0.000187), f1_tau 0.331063 (0.082602), f1_2tau 0.518382 (0.124788)
Test Step 000700/005472 (000700) cd 0.000863 (0.000196), f1_tau 0.336034 (0.081476), f1_2tau 0.521561 (0.123612)
Test Step 000750/005472 (000750) cd 0.000865 (0.000196), f1_tau 0.341548 (0.081505), f1_2tau 0.525472 (0.123603)
Test Step 000800/005472 (000800) cd 0.000870 (0.000197), f1_tau 0.344795 (0.081051), f1_2tau 0.527352 (0.123142)
Test Step 000850/005472 (000850) cd 0.000875 (0.000277), f1_tau 0.346266 (0.107352), f1_2tau 0.528098 (0.162908)
Test Step 000900/005472 (000900) cd 0.000879 (0.000272), f1_tau 0.345730 (0.107112), f1_2tau 0.527161 (0.162647)
Test Step 000950/005472 (000950) cd 0.000878 (0.000268), f1_tau 0.346327 (0.107658), f1_2tau 0.527791 (0.163407)
Test Step 001000/005472 (001000) cd 0.000878 (0.000268), f1_tau 0.346753 (0.107854), f1_2tau 0.528131 (0.163649)
Test Step 001050/005472 (001050) cd 0.000879 (0.000267), f1_tau 0.347269 (0.108011), f1_2tau 0.528623 (0.163852)
Test Step 001100/005472 (001100) cd 0.000881 (0.000267), f1_tau 0.347606 (0.108073), f1_2tau 0.528813 (0.163904)
Test Step 001150/005472 (001150) cd 0.000878 (0.000266), f1_tau 0.347855 (0.108100), f1_2tau 0.529227 (0.164005)
Test Step 001200/005472 (001200) cd 0.000879 (0.000266), f1_tau 0.348108 (0.108126), f1_2tau 0.529518 (0.164059)
Test Step 001250/005472 (001250) cd 0.000880 (0.000266), f1_tau 0.348180 (0.108110), f1_2tau 0.529518 (0.164040)
Test Step 001300/005472 (001300) cd 0.000885 (0.000267), f1_tau 0.348171 (0.108081), f1_2tau 0.529259 (0.163971)
Test Step 001350/005472 (001350) cd 0.000884 (0.000266), f1_tau 0.348352 (0.108095), f1_2tau 0.529394 (0.163988)
Test Step 001400/005472 (001400) cd 0.000884 (0.000266), f1_tau 0.348514 (0.108106), f1_2tau 0.529460 (0.163991)
Test Step 001450/005472 (001450) cd 0.000885 (0.000266), f1_tau 0.348584 (0.108100), f1_2tau 0.529458 (0.163983)
Test Step 001500/005472 (001500) cd 0.000887 (0.000267), f1_tau 0.348511 (0.108072), f1_2tau 0.529295 (0.163949)
Test Step 001550/005472 (001550) cd 0.000886 (0.000266), f1_tau 0.348517 (0.108060), f1_2tau 0.529336 (0.163950)
Test Step 001600/005472 (001600) cd 0.000887 (0.000266), f1_tau 0.348710 (0.108079), f1_2tau 0.529364 (0.163950)
Test Step 001650/005472 (001650) cd 0.000888 (0.000267), f1_tau 0.348632 (0.108056), f1_2tau 0.529257 (0.163930)
Test Step 001700/005472 (001700) cd 0.000887 (0.000336), f1_tau 0.349632 (0.137344), f1_2tau 0.530143 (0.206413)
Test Step 001750/005472 (001750) cd 0.000890 (0.000339), f1_tau 0.350009 (0.136554), f1_2tau 0.530242 (0.205595)
Test Step 001800/005472 (001800) cd 0.000891 (0.000339), f1_tau 0.350531 (0.136527), f1_2tau 0.530467 (0.205526)
Test Step 001850/005472 (001850) cd 0.000889 (0.000401), f1_tau 0.356245 (0.180758), f1_2tau 0.534698 (0.259130)
Test Step 001900/005472 (001900) cd 0.000890 (0.000406), f1_tau 0.361334 (0.179762), f1_2tau 0.538344 (0.258203)
Test Step 001950/005472 (001950) cd 0.000887 (0.000403), f1_tau 0.366941 (0.180241), f1_2tau 0.542522 (0.258644)
Test Step 002000/005472 (002000) cd 0.000885 (0.000403), f1_tau 0.371846 (0.180147), f1_2tau 0.546129 (0.258577)
Test Step 002050/005472 (002050) cd 0.000882 (0.000402), f1_tau 0.376703 (0.180214), f1_2tau 0.549826 (0.258707)
Test Step 002100/005472 (002100) cd 0.000882 (0.000457), f1_tau 0.381044 (0.210682), f1_2tau 0.552985 (0.304958)
Test Step 002150/005472 (002150) cd 0.000889 (0.000491), f1_tau 0.378738 (0.201837), f1_2tau 0.550791 (0.293935)
Test Step 002200/005472 (002200) cd 0.000892 (0.000488), f1_tau 0.376519 (0.201786), f1_2tau 0.548745 (0.293988)
Test Step 002250/005472 (002250) cd 0.000896 (0.000486), f1_tau 0.374501 (0.201886), f1_2tau 0.546951 (0.294192)
Test Step 002300/005472 (002300) cd 0.000900 (0.000486), f1_tau 0.372673 (0.202027), f1_2tau 0.545188 (0.294253)
Test Step 002350/005472 (002350) cd 0.000910 (0.000592), f1_tau 0.372267 (0.229430), f1_2tau 0.544255 (0.332929)
Test Step 002400/005472 (002400) cd 0.000918 (0.000588), f1_tau 0.372697 (0.230871), f1_2tau 0.544186 (0.334427)
Test Step 002450/005472 (002450) cd 0.000930 (0.000594), f1_tau 0.372576 (0.230659), f1_2tau 0.543545 (0.334181)
Test Step 002500/005472 (002500) cd 0.000939 (0.000593), f1_tau 0.372610 (0.230701), f1_2tau 0.543061 (0.334187)
Test Step 002550/005472 (002550) cd 0.000949 (0.000594), f1_tau 0.372117 (0.230309), f1_2tau 0.542121 (0.333815)
Test Step 002600/005472 (002600) cd 0.000958 (0.000637), f1_tau 0.371997 (0.262802), f1_2tau 0.541676 (0.381437)
Test Step 002650/005472 (002650) cd 0.000950 (0.000638), f1_tau 0.373827 (0.265754), f1_2tau 0.543993 (0.384290)
Test Step 002700/005472 (002700) cd 0.000944 (0.000640), f1_tau 0.374980 (0.264809), f1_2tau 0.545762 (0.383620)
Test Step 002750/005472 (002750) cd 0.000943 (0.000706), f1_tau 0.376705 (0.304239), f1_2tau 0.547313 (0.433763)
Test Step 002800/005472 (002800) cd 0.000939 (0.000704), f1_tau 0.379356 (0.304541), f1_2tau 0.549495 (0.434177)
Test Step 002850/005472 (002850) cd 0.000936 (0.000704), f1_tau 0.381848 (0.304491), f1_2tau 0.551509 (0.434105)
Test Step 002900/005472 (002900) cd 0.000933 (0.000704), f1_tau 0.384461 (0.304731), f1_2tau 0.553649 (0.434321)
Test Step 002950/005472 (002950) cd 0.000930 (0.000705), f1_tau 0.386539 (0.304418), f1_2tau 0.555308 (0.434034)
Test Step 003000/005472 (003000) cd 0.000926 (0.000704), f1_tau 0.388882 (0.304497), f1_2tau 0.557258 (0.434138)
Test Step 003050/005472 (003050) cd 0.000925 (0.000705), f1_tau 0.390753 (0.304268), f1_2tau 0.558700 (0.433892)
Test Step 003100/005472 (003100) cd 0.000924 (0.000706), f1_tau 0.392599 (0.304122), f1_2tau 0.560100 (0.433714)
Test Step 003150/005472 (003150) cd 0.000923 (0.000707), f1_tau 0.394454 (0.304049), f1_2tau 0.561554 (0.433633)
Test Step 003200/005472 (003200) cd 0.000922 (0.000707), f1_tau 0.395983 (0.303852), f1_2tau 0.562693 (0.433430)
Test Step 003250/005472 (003250) cd 0.000919 (0.000772), f1_tau 0.396655 (0.328910), f1_2tau 0.563263 (0.472232)
Test Step 003300/005472 (003300) cd 0.000920 (0.000780), f1_tau 0.395491 (0.328617), f1_2tau 0.562236 (0.471788)
Test Step 003350/005472 (003350) cd 0.000920 (0.000778), f1_tau 0.394662 (0.329205), f1_2tau 0.561568 (0.472409)
Test Step 003400/005472 (003400) cd 0.000921 (0.000779), f1_tau 0.393580 (0.329025), f1_2tau 0.560651 (0.472257)
Test Step 003450/005472 (003450) cd 0.000921 (0.000779), f1_tau 0.392665 (0.329089), f1_2tau 0.559896 (0.472336)
Test Step 003500/005472 (003500) cd 0.000922 (0.000780), f1_tau 0.391728 (0.329082), f1_2tau 0.559084 (0.472307)
Test Step 003550/005472 (003550) cd 0.000924 (0.000781), f1_tau 0.390718 (0.328993), f1_2tau 0.558103 (0.472125)
Test Step 003600/005472 (003600) cd 0.000924 (0.000781), f1_tau 0.389886 (0.329039), f1_2tau 0.557437 (0.472206)
Test Step 003650/005472 (003650) cd 0.000924 (0.000780), f1_tau 0.389006 (0.329028), f1_2tau 0.556689 (0.472201)
Test Step 003700/005472 (003700) cd 0.000924 (0.000780), f1_tau 0.388269 (0.329091), f1_2tau 0.556083 (0.472271)
Test Step 003750/005472 (003750) cd 0.000924 (0.000780), f1_tau 0.387446 (0.329083), f1_2tau 0.555377 (0.472263)
Test Step 003800/005472 (003800) cd 0.000925 (0.000780), f1_tau 0.386765 (0.329138), f1_2tau 0.554742 (0.472284)
Test Step 003850/005472 (003850) cd 0.000925 (0.000780), f1_tau 0.386113 (0.329190), f1_2tau 0.554242 (0.472357)
Test Step 003900/005472 (003900) cd 0.000926 (0.000780), f1_tau 0.385320 (0.329164), f1_2tau 0.553455 (0.472286)
Test Step 003950/005472 (003950) cd 0.000927 (0.000781), f1_tau 0.384450 (0.329100), f1_2tau 0.552647 (0.472207)
Test Step 004000/005472 (004000) cd 0.000926 (0.000780), f1_tau 0.383865 (0.329149), f1_2tau 0.552187 (0.472269)
Test Step 004050/005472 (004050) cd 0.000925 (0.000780), f1_tau 0.383264 (0.329182), f1_2tau 0.551704 (0.472311)
Test Step 004100/005472 (004100) cd 0.000927 (0.000780), f1_tau 0.382693 (0.329216), f1_2tau 0.551207 (0.472339)
Test Step 004150/005472 (004150) cd 0.000928 (0.000780), f1_tau 0.381937 (0.329177), f1_2tau 0.550508 (0.472289)
Test Step 004200/005472 (004200) cd 0.000928 (0.000780), f1_tau 0.381376 (0.329201), f1_2tau 0.550024 (0.472310)
Test Step 004250/005472 (004250) cd 0.000928 (0.000780), f1_tau 0.380702 (0.329182), f1_2tau 0.549453 (0.472298)
Test Step 004300/005472 (004300) cd 0.000928 (0.000825), f1_tau 0.380207 (0.360450), f1_2tau 0.549072 (0.518925)
Test Step 004350/005472 (004350) cd 0.000923 (0.000823), f1_tau 0.380565 (0.360771), f1_2tau 0.549837 (0.519545)
Test Step 004400/005472 (004400) cd 0.000920 (0.000825), f1_tau 0.380798 (0.360425), f1_2tau 0.550441 (0.519140)
Test Step 004450/005472 (004450) cd 0.000916 (0.000824), f1_tau 0.381140 (0.360546), f1_2tau 0.551157 (0.519264)
Test Step 004500/005472 (004500) cd 0.000912 (0.000824), f1_tau 0.381530 (0.360702), f1_2tau 0.551927 (0.519446)
Test Step 004550/005472 (004550) cd 0.000908 (0.000824), f1_tau 0.381777 (0.360615), f1_2tau 0.552536 (0.519360)
Test Step 004600/005472 (004600) cd 0.000904 (0.000824), f1_tau 0.382108 (0.360659), f1_2tau 0.553241 (0.519429)
Test Step 004650/005472 (004650) cd 0.000901 (0.000825), f1_tau 0.382313 (0.360571), f1_2tau 0.553765 (0.519312)
Test Step 004700/005472 (004700) cd 0.000898 (0.000825), f1_tau 0.382548 (0.360535), f1_2tau 0.554324 (0.519265)
Test Step 004750/005472 (004750) cd 0.000894 (0.000825), f1_tau 0.382732 (0.360471), f1_2tau 0.554848 (0.519209)
Test Step 004800/005472 (004800) cd 0.000891 (0.000825), f1_tau 0.382905 (0.360414), f1_2tau 0.555327 (0.519140)
Test Step 004850/005472 (004850) cd 0.000888 (0.000825), f1_tau 0.383131 (0.360405), f1_2tau 0.555838 (0.519111)
Test Step 004900/005472 (004900) cd 0.000885 (0.000825), f1_tau 0.383270 (0.360347), f1_2tau 0.556282 (0.519052)
Test Step 004950/005472 (004950) cd 0.000882 (0.000825), f1_tau 0.383511 (0.360357), f1_2tau 0.556831 (0.519068)
Test Step 005000/005472 (005000) cd 0.000879 (0.000825), f1_tau 0.383769 (0.360379), f1_2tau 0.557394 (0.519095)
Test Step 005050/005472 (005050) cd 0.000876 (0.000825), f1_tau 0.384065 (0.360420), f1_2tau 0.557961 (0.519126)
Test Step 005100/005472 (005100) cd 0.000873 (0.000825), f1_tau 0.384267 (0.360412), f1_2tau 0.558436 (0.519114)
Test Step 005150/005472 (005150) cd 0.000870 (0.000825), f1_tau 0.384467 (0.360407), f1_2tau 0.558892 (0.519099)
Test Step 005200/005472 (005200) cd 0.000868 (0.000825), f1_tau 0.384670 (0.360405), f1_2tau 0.559354 (0.519093)
Test Step 005250/005472 (005250) cd 0.000866 (0.000894), f1_tau 0.384982 (0.393539), f1_2tau 0.559736 (0.564057)
Test Step 005300/005472 (005300) cd 0.000866 (0.000894), f1_tau 0.385602 (0.394631), f1_2tau 0.560181 (0.565283)
Test Step 005350/005472 (005350) cd 0.000867 (0.000894), f1_tau 0.386141 (0.394581), f1_2tau 0.560534 (0.565203)
Test Step 005400/005472 (005400) cd 0.000868 (0.000896), f1_tau 0.386379 (0.393852), f1_2tau 0.560580 (0.564439)
Test Step 005450/005472 (005450) cd 0.000867 (0.000894), f1_tau 0.386974 (0.394140), f1_2tau 0.560987 (0.564710)
Test [005473] cd: 0.000895
Test [005473] f1_tau: 0.393849
Test [005473] f1_2tau: 0.564412







v.shape =>  torch.Size([8, 618, 8])
new_mv before gp_layer torch.Size([8, 618, 8])
new_mv after gp_layer torch.Size([8, 618, 618, 8])
output att_prj : torch.Size([8, 618, 618, 1])
attn_scores ->  torch.Size([8, 618, 618])
x2_att => torch.Size([8, 618, 8])
















okay bene ora vorrei modificare questo codice per migliorarlo utilizzando la geometric algebra hai qualche idea da propormi? Che ne pensi modificare i livelli GCN in modo che siano E(3)-equivarianti, introducendo meccanismi di attenzione basati su GA. I GA consentono di gestire rotazioni e traslazioni in modo più naturale, quindi si potrebbero costruire livelli GCN che siano intrinsecamente equivarianti a tali trasformazioni, migliorando la capacità della rete di generalizzare tra diverse pose e scale dell'oggetto nell'immagine di input.
Implementare i livelli di attenzione utilizzando prodotti geometrici per calcolare i punteggi di attenzione tra i vertici della mesh, considerando sia le posizioni che le relazioni geometriche (come le direzioni dei bordi o le normali della superficie) rappresentate come multivettori.  Mi puoi aiutarlo a fare? Ho bisogno anche di capire se cio' funzionerebbe oppure no!
tuttavia sentiti libero di espormi le critiche di questo approccio e nel caso migliorarlo.

import torch
import torch.nn as nn
import torch.nn.functional as F

import sys # for stopping whenever I want!

from models.backbones import get_backbone
from models.layers.gbottleneck import GBottleneck
from models.layers.gconv import GConv
from models.layers.gpooling import GUnpooling
from models.layers.gprojection import GProjection


class P2MModel(nn.Module):

    def __init__(self, options, ellipsoid, camera_f, camera_c, mesh_pos):
        super(P2MModel, self).__init__()

        self.hidden_dim = options.hidden_dim
        self.coord_dim = options.coord_dim
        self.last_hidden_dim = options.last_hidden_dim
        self.init_pts = nn.Parameter(ellipsoid.coord, requires_grad=False)
        self.gconv_activation = options.gconv_activation

        self.nn_encoder, self.nn_decoder = get_backbone(options)
        self.features_dim = self.nn_encoder.features_dim + self.coord_dim

        self.gcns = nn.ModuleList([
            GBottleneck(6, self.features_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[0], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[1], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.last_hidden_dim,
                        ellipsoid.adj_mat[2], activation=self.gconv_activation)
        ])

        self.unpooling = nn.ModuleList([
            GUnpooling(ellipsoid.unpool_idx[0]),
            GUnpooling(ellipsoid.unpool_idx[1])
        ])

        # if options.align_with_tensorflow:
        #     self.projection = GProjection
        # else:
        #     self.projection = GProjection
        self.projection = GProjection(mesh_pos, camera_f, camera_c, bound=options.z_threshold,
                                      tensorflow_compatible=options.align_with_tensorflow)

        self.gconv = GConv(in_features=self.last_hidden_dim, out_features=self.coord_dim,
                           adj_mat=ellipsoid.adj_mat[2])

    def forward(self, img):
        batch_size = img.size(0)
        img_feats = self.nn_encoder(img)
        img_shape = self.projection.image_feature_shape(img)

        init_pts = self.init_pts.data.unsqueeze(0).expand(batch_size, -1, -1)


        # GCN Block 1
        x = self.projection(img_shape, img_feats, init_pts)
  
        x1, x_hidden = self.gcns[0](x)
        # x1 -> new coordinate of vertices => ex: torch.Size([8, 156, 3])
        # x_hidden -> feature learned during gcn => ex:  torch.Size([8, 156, 192])
    

        # before deformation 2
        x1_up = self.unpooling[0](x1)

        # GCN Block 2
        x = self.projection(img_shape, img_feats, x1)
       
        x = self.unpooling[0](torch.cat([x, x_hidden], 2))
  
        # after deformation 2
        x2, x_hidden = self.gcns[1](x)


        # before deformation 3
        x2_up = self.unpooling[1](x2)
 

        # GCN Block 3
        x = self.projection(img_shape, img_feats, x2)

        x = self.unpooling[1](torch.cat([x, x_hidden], 2))
   
        x3, _ = self.gcns[2](x)
    
        if self.gconv_activation:
            x3 = F.relu(x3)


        
        # after deformation 3
        x3 = self.gconv(x3)

        # sys.exit()

        if self.nn_decoder is not None:
            reconst = self.nn_decoder(img_feats)
        else:
            reconst = None

        return {
            "pred_coord": [x1, x2, x3],
            "pred_coord_before_deform": [init_pts, x1_up, x2_up],
            "reconst": reconst
        }




#TODO TEST SELF ATTENTION 
# class CliffordAttention(nn.Module):
#     def __init__(self, in_features, out_features, heads, algebra):
#         super(CliffordAttention, self).__init__()
#         self.in_features = in_features
#         self.out_features = out_features
#         self.heads = heads
#         self.algebra = algebra
#         self.scale = (in_features // heads) ** -0.5
        
#         self.to_queries = nn.Linear(in_features, out_features, bias=False)
#         self.to_keys = nn.Linear(in_features, out_features, bias=False)
#         self.to_values = nn.Linear(in_features, out_features, bias=False)
#         self.unify_heads = nn.Linear(out_features, in_features)

#     def forward(self, x):
#         b, n, _ = x.size()  # batch size, number of vertices, features
        
#         queries = self.to_queries(x).view(b, n, self.heads, self.out_features // self.heads)
#         keys = self.to_keys(x).view(b, n, self.heads, self.out_features // self.heads)
#         values = self.to_values(x).view(b, n, self.heads, self.out_features // self.heads)

#         # Calculate the geometric product as attention scores
#         # scores: (b, heads, n, n)
#         scores = self.algebra.geometric_product(queries, keys.transpose(-2, -1)) * self.scale
#         scores = F.softmax(scores, dim=-1)

#         # Attention-weighted sum of values
#         attended_values = torch.einsum('bhij,bhjd->bhid', scores, values)
#         attended_values = attended_values.contiguous().view(b, n, self.out_features)

#         return self.unify_heads(attended_values)






DOMANDE:
Originale:
Epoch 001, Step 000050/2408065, Time elapsed 0:00:21.297852, Loss 0.063401863 (0.146967320)
Epoch 001, Step 000100/2408065, Time elapsed 0:00:38.262325, Loss 0.040274613 (0.099145336)
Epoch 001, Step 000150/2408065, Time elapsed 0:00:55.270860, Loss 0.035927325 (0.078594115)
Epoch 001, Step 000200/2408065, Time elapsed 0:01:12.321128, Loss 0.029634126 (0.066952466)







PRIMA IMPLEMENTAZIONE:
Risultati:
Epoch 001, Step 000050/2408065, Time elapsed 0:00:26.353997, Loss 0.020890592 (0.644999308)
Epoch 001, Step 000050/2408065, Time elapsed 0:00:26.353997, Loss 0.020890592 (0.644999308)
Epoch 001, Step 000100/2408065, Time elapsed 0:00:46.694301, Loss 0.014576641 (0.330137176)
Epoch 001, Step 000100/2408065, Time elapsed 0:00:46.694301, Loss 0.014576641 (0.330137176)
Epoch 001, Step 000150/2408065, Time elapsed 0:01:06.952115, Loss 0.009921547 (0.223488397)
Epoch 001, Step 000150/2408065, Time elapsed 0:01:06.952115, Loss 0.009921547 (0.223488397)
Epoch 001, Step 000200/2408065, Time elapsed 0:01:27.386055, Loss 0.008196960 (0.169704465)
Epoch 001, Step 000200/2408065, Time elapsed 0:01:27.386055, Loss 0.008196960 (0.169704465)
Epoch 001, Step 000250/2408065, Time elapsed 0:01:48.005758, Loss 0.005831062 (0.137338186)
Epoch 001, Step 000250/2408065, Time elapsed 0:01:48.005758, Loss 0.005831062 (0.137338186)
Epoch 001, Step 000300/2408065, Time elapsed 0:02:08.652820, Loss 0.007751753 (0.115638513)
Epoch 001, Step 000300/2408065, Time elapsed 0:02:08.652820, Loss 0.007751753 (0.115638513)
Epoch 001, Step 000350/2408065, Time elapsed 0:02:28.872281, Loss 0.006007112 (0.100049831)
Epoch 001, Step 000350/2408065, Time elapsed 0:02:28.872281, Loss 0.006007112 (0.100049831)
Epoch 001, Step 000400/2408065, Time elapsed 0:02:49.337993, Loss 0.012860646 (0.088330669)
Epoch 001, Step 000400/2408065, Time elapsed 0:02:49.337993, Loss 0.012860646 (0.088330669)
Epoch 001, Step 000450/2408065, Time elapsed 0:03:09.697376, Loss 0.007826809 (0.079310426)
Epoch 001, Step 000450/2408065, Time elapsed 0:03:09.697376, Loss 0.007826809 (0.079310426)
Epoch 001, Step 000500/2408065, Time elapsed 0:03:30.841069, Loss 0.006592039 (0.071970700)
Epoch 001, Step 000500/2408065, Time elapsed 0:03:30.841069, Loss 0.006592039 (0.071970700)
Epoch 001, Step 000550/2408065, Time elapsed 0:03:51.677433, Loss 0.005201591 (0.065915697)
Epoch 001, Step 000550/2408065, Time elapsed 0:03:51.677433, Loss 0.005201591 (0.065915697)
Epoch 001, Step 000600/2408065, Time elapsed 0:04:12.907413, Loss 0.006522654 (0.060877195)
Epoch 001, Step 000600/2408065, Time elapsed 0:04:12.907413, Loss 0.006522654 (0.060877195)
Epoch 001, Step 000650/2408065, Time elapsed 0:04:34.720724, Loss 0.006344048 (0.056592190)
Epoch 001, Step 000650/2408065, Time elapsed 0:04:34.720724, Loss 0.006344048 (0.056592190)
Epoch 001, Step 000700/2408065, Time elapsed 0:04:56.667539, Loss 0.005635871 (0.052978157)
Epoch 001, Step 000700/2408065, Time elapsed 0:04:56.667539, Loss 0.005635871 (0.052978157)
Epoch 001, Step 000750/2408065, Time elapsed 0:05:17.144163, Loss 0.008414846 (0.049751953)
Epoch 001, Step 000750/2408065, Time elapsed 0:05:17.144163, Loss 0.008414846 (0.049751953)
Epoch 001, Step 000800/2408065, Time elapsed 0:05:37.481386, Loss 0.003156736 (0.046946236)
Epoch 001, Step 000800/2408065, Time elapsed 0:05:37.481386, Loss 0.003156736 (0.046946236)
Epoch 001, Step 000850/2408065, Time elapsed 0:05:58.365536, Loss 0.005476789 (0.044518623)
Epoch 001, Step 000850/2408065, Time elapsed 0:05:58.365536, Loss 0.005476789 (0.044518623)
Epoch 001, Step 000900/2408065, Time elapsed 0:06:19.018692, Loss 0.005858089 (0.042372420)
Epoch 001, Step 000900/2408065, Time elapsed 0:06:19.018692, Loss 0.005858089 (0.042372420)
Epoch 001, Step 000950/2408065, Time elapsed 0:06:39.675327, Loss 0.005319768 (0.040440207)
Epoch 001, Step 000950/2408065, Time elapsed 0:06:39.675327, Loss 0.005319768 (0.040440207)
Epoch 001, Step 001000/2408065, Time elapsed 0:07:00.597302, Loss 0.004982734 (0.038700271)
Epoch 001, Step 001000/2408065, Time elapsed 0:07:00.597302, Loss 0.004982734 (0.038700271)
Epoch 001, Step 001050/2408065, Time elapsed 0:07:20.974822, Loss 0.004745572 (0.037113971)
Epoch 001, Step 001050/2408065, Time elapsed 0:07:20.974822, Loss 0.004745572 (0.037113971)
Epoch 001, Step 001100/2408065, Time elapsed 0:07:41.563861, Loss 0.006541237 (0.035694480)
Epoch 001, Step 001100/2408065, Time elapsed 0:07:41.563861, Loss 0.006541237 (0.035694480)
Epoch 001, Step 001150/2408065, Time elapsed 0:08:02.290247, Loss 0.006944756 (0.034607561)
Epoch 001, Step 001150/2408065, Time elapsed 0:08:02.290247, Loss 0.006944756 (0.034607561)
Epoch 001, Step 001200/2408065, Time elapsed 0:08:22.778493, Loss 0.006091320 (0.033403138)
Epoch 001, Step 001200/2408065, Time elapsed 0:08:22.778493, Loss 0.006091320 (0.033403138)
Epoch 001, Step 001250/2408065, Time elapsed 0:08:43.043868, Loss 0.005766677 (0.032280327)
Epoch 001, Step 001250/2408065, Time elapsed 0:08:43.043868, Loss 0.005766677 (0.032280327)
Epoch 001, Step 001300/2408065, Time elapsed 0:09:04.122444, Loss 0.004428875 (0.031249174)
Epoch 001, Step 001300/2408065, Time elapsed 0:09:04.122444, Loss 0.004428875 (0.031249174)
Epoch 001, Step 001350/2408065, Time elapsed 0:09:24.604130, Loss 0.003925713 (0.030264111)
Epoch 001, Step 001350/2408065, Time elapsed 0:09:24.604130, Loss 0.003925713 (0.030264111)
Epoch 001, Step 001400/2408065, Time elapsed 0:09:45.268842, Loss 0.004516981 (0.029360349)
Epoch 001, Step 001400/2408065, Time elapsed 0:09:45.268842, Loss 0.004516981 (0.029360349)
Epoch 001, Step 001450/2408065, Time elapsed 0:10:06.422050, Loss 0.004696125 (0.028519083)
Epoch 001, Step 001450/2408065, Time elapsed 0:10:06.422050, Loss 0.004696125 (0.028519083)
Epoch 001, Step 001500/2408065, Time elapsed 0:10:26.873225, Loss 0.008649380 (0.027745345)
Epoch 001, Step 001500/2408065, Time elapsed 0:10:26.873225, Loss 0.008649380 (0.027745345)
Epoch 001, Step 001550/2408065, Time elapsed 0:10:47.387225, Loss 0.005486036 (0.027010559)
Epoch 001, Step 001550/2408065, Time elapsed 0:10:47.387225, Loss 0.005486036 (0.027010559)
Epoch 001, Step 001600/2408065, Time elapsed 0:11:08.187647, Loss 0.004593367 (0.026316742)
Epoch 001, Step 001600/2408065, Time elapsed 0:11:08.187647, Loss 0.004593367 (0.026316742)
Epoch 001, Step 001650/2408065, Time elapsed 0:11:28.349208, Loss 0.004399175 (0.025664147)
Epoch 001, Step 001650/2408065, Time elapsed 0:11:28.349208, Loss 0.004399175 (0.025664147)
Epoch 001, Step 001700/2408065, Time elapsed 0:11:48.892556, Loss 0.005454075 (0.025047672)
Epoch 001, Step 001700/2408065, Time elapsed 0:11:48.892556, Loss 0.005454075 (0.025047672)
Epoch 001, Step 001750/2408065, Time elapsed 0:12:09.279576, Loss 0.003958442 (0.024481131)
Epoch 001, Step 001750/2408065, Time elapsed 0:12:09.279576, Loss 0.003958442 (0.024481131)
Epoch 001, Step 001800/2408065, Time elapsed 0:12:29.621796, Loss 0.006247469 (0.024299252)
Epoch 001, Step 001800/2408065, Time elapsed 0:12:29.621796, Loss 0.006247469 (0.024299252)
Epoch 001, Step 001850/2408065, Time elapsed 0:12:50.176397, Loss 0.005547092 (0.023890003)
Epoch 001, Step 001850/2408065, Time elapsed 0:12:50.176397, Loss 0.005547092 (0.023890003)
Epoch 001, Step 001900/2408065, Time elapsed 0:13:11.796757, Loss 0.005096584 (0.023373697)
Epoch 001, Step 001900/2408065, Time elapsed 0:13:11.796757, Loss 0.005096584 (0.023373697)
Epoch 001, Step 001950/2408065, Time elapsed 0:13:32.867787, Loss 0.002084842 (0.022882086)
Epoch 001, Step 001950/2408065, Time elapsed 0:13:32.867787, Loss 0.002084842 (0.022882086)
Epoch 001, Step 002000/2408065, Time elapsed 0:13:54.126214, Loss 0.001927833 (0.022387855)
Epoch 001, Step 002000/2408065, Time elapsed 0:13:54.126214, Loss 0.001927833 (0.022387855)
Epoch 001, Step 002050/2408065, Time elapsed 0:14:15.388003, Loss 0.003457858 (0.021914854)
Epoch 001, Step 002050/2408065, Time elapsed 0:14:15.388003, Loss 0.003457858 (0.021914854)
Epoch 001, Step 002100/2408065, Time elapsed 0:14:37.162340, Loss 0.007695934 (0.021480966)
Epoch 001, Step 002100/2408065, Time elapsed 0:14:37.162340, Loss 0.007695934 (0.021480966)
Epoch 001, Step 002150/2408065, Time elapsed 0:14:57.603551, Loss 0.005532159 (0.021129325)
Epoch 001, Step 002150/2408065, Time elapsed 0:14:57.603551, Loss 0.005532159 (0.021129325)
Epoch 001, Step 002200/2408065, Time elapsed 0:15:18.080721, Loss 0.006252571 (0.020775699)
Epoch 001, Step 002200/2408065, Time elapsed 0:15:18.080721, Loss 0.006252571 (0.020775699)
Epoch 001, Step 002250/2408065, Time elapsed 0:15:38.623683, Loss 0.005839349 (0.020432217)
Epoch 001, Step 002250/2408065, Time elapsed 0:15:38.623683, Loss 0.005839349 (0.020432217)
Epoch 001, Step 002300/2408065, Time elapsed 0:16:00.277647, Loss 0.009426109 (0.020100355)
Epoch 001, Step 002300/2408065, Time elapsed 0:16:00.277647, Loss 0.009426109 (0.020100355)
Epoch 001, Step 002350/2408065, Time elapsed 0:16:21.353466, Loss 0.022893922 (0.020067691)
Epoch 001, Step 002350/2408065, Time elapsed 0:16:21.353466, Loss 0.022893922 (0.020067691)
Epoch 001, Step 002400/2408065, Time elapsed 0:16:41.874905, Loss 0.009438138 (0.020177201)
Epoch 001, Step 002400/2408065, Time elapsed 0:16:41.874905, Loss 0.009438138 (0.020177201)
Epoch 001, Step 002450/2408065, Time elapsed 0:17:02.454798, Loss 0.003263107 (0.019906701)
Epoch 001, Step 002450/2408065, Time elapsed 0:17:02.454798, Loss 0.003263107 (0.019906701)
Epoch 001, Step 002500/2408065, Time elapsed 0:17:22.848288, Loss 0.004463271 (0.019623161)
Epoch 001, Step 002500/2408065, Time elapsed 0:17:22.848288, Loss 0.004463271 (0.019623161)
Epoch 001, Step 002550/2408065, Time elapsed 0:17:43.771977, Loss 0.004727659 (0.019350031)
Epoch 001, Step 002550/2408065, Time elapsed 0:17:43.771977, Loss 0.004727659 (0.019350031)
Epoch 001, Step 002600/2408065, Time elapsed 0:18:04.518661, Loss 0.003493302 (0.019092458)
Epoch 001, Step 002600/2408065, Time elapsed 0:18:04.518661, Loss 0.003493302 (0.019092458)
Epoch 001, Step 002650/2408065, Time elapsed 0:18:24.991138, Loss 0.003711842 (0.018795535)
Epoch 001, Step 002650/2408065, Time elapsed 0:18:24.991138, Loss 0.003711842 (0.018795535)
Epoch 001, Step 002700/2408065, Time elapsed 0:18:45.522768, Loss 0.002780915 (0.018502599)
Epoch 001, Step 002700/2408065, Time elapsed 0:18:45.522768, Loss 0.002780915 (0.018502599)
Epoch 001, Step 002750/2408065, Time elapsed 0:19:07.523695, Loss 0.004368688 (0.018231841)
Epoch 001, Step 002750/2408065, Time elapsed 0:19:07.523695, Loss 0.004368688 (0.018231841)
Epoch 001, Step 002800/2408065, Time elapsed 0:19:28.595368, Loss 0.002580677 (0.017959547)
Epoch 001, Step 002800/2408065, Time elapsed 0:19:28.595368, Loss 0.002580677 (0.017959547)
Epoch 001, Step 002850/2408065, Time elapsed 0:19:49.360916, Loss 0.004179907 (0.017693385)
Epoch 001, Step 002850/2408065, Time elapsed 0:19:49.360916, Loss 0.004179907 (0.017693385)
Epoch 001, Step 002900/2408065, Time elapsed 0:20:09.806633, Loss 0.002093546 (0.017435395)
Epoch 001, Step 002900/2408065, Time elapsed 0:20:09.806633, Loss 0.002093546 (0.017435395)
Epoch 001, Step 002950/2408065, Time elapsed 0:20:30.183407, Loss 0.002424107 (0.017186537)
Epoch 001, Step 002950/2408065, Time elapsed 0:20:30.183407, Loss 0.002424107 (0.017186537)
Epoch 001, Step 003000/2408065, Time elapsed 0:20:50.766161, Loss 0.002240742 (0.016944680)
Epoch 001, Step 003000/2408065, Time elapsed 0:20:50.766161, Loss 0.002240742 (0.016944680)
Epoch 001, Step 003050/2408065, Time elapsed 0:21:11.864813, Loss 0.005086914 (0.016717025)
Epoch 001, Step 003050/2408065, Time elapsed 0:21:11.864813, Loss 0.005086914 (0.016717025)
Epoch 001, Step 003100/2408065, Time elapsed 0:21:33.901054, Loss 0.004328846 (0.016496887)
Epoch 001, Step 003100/2408065, Time elapsed 0:21:33.901054, Loss 0.004328846 (0.016496887)
Epoch 001, Step 003150/2408065, Time elapsed 0:21:55.364328, Loss 0.002438635 (0.016286699)
Epoch 001, Step 003150/2408065, Time elapsed 0:21:55.364328, Loss 0.002438635 (0.016286699)
Epoch 001, Step 003200/2408065, Time elapsed 0:22:16.968154, Loss 0.002233215 (0.016077467)
Epoch 001, Step 003200/2408065, Time elapsed 0:22:16.968154, Loss 0.002233215 (0.016077467)
Epoch 001, Step 003250/2408065, Time elapsed 0:22:38.002762, Loss 0.006026410 (0.015889165)
Epoch 001, Step 003250/2408065, Time elapsed 0:22:38.002762, Loss 0.006026410 (0.015889165)
Epoch 001, Step 003300/2408065, Time elapsed 0:22:59.252201, Loss 0.005581018 (0.015722345)
Epoch 001, Step 003300/2408065, Time elapsed 0:22:59.252201, Loss 0.005581018 (0.015722345)
Epoch 001, Step 003350/2408065, Time elapsed 0:23:19.739377, Loss 0.004158022 (0.015554686)
Epoch 001, Step 003350/2408065, Time elapsed 0:23:19.739377, Loss 0.004158022 (0.015554686)
Epoch 001, Step 003400/2408065, Time elapsed 0:23:39.967556, Loss 0.003981418 (0.015396155)
Epoch 001, Step 003400/2408065, Time elapsed 0:23:39.967556, Loss 0.003981418 (0.015396155)
Epoch 001, Step 003450/2408065, Time elapsed 0:24:00.281475, Loss 0.004163730 (0.015244488)
Epoch 001, Step 003450/2408065, Time elapsed 0:24:00.281475, Loss 0.004163730 (0.015244488)
Epoch 001, Step 003500/2408065, Time elapsed 0:24:20.801722, Loss 0.004977354 (0.015089607)
Epoch 001, Step 003500/2408065, Time elapsed 0:24:20.801722, Loss 0.004977354 (0.015089607)
Epoch 001, Step 003550/2408065, Time elapsed 0:24:41.385280, Loss 0.006294376 (0.014947621)
Epoch 001, Step 003550/2408065, Time elapsed 0:24:41.385280, Loss 0.006294376 (0.014947621)
Epoch 001, Step 003600/2408065, Time elapsed 0:25:01.971715, Loss 0.005990819 (0.014800844)
Epoch 001, Step 003600/2408065, Time elapsed 0:25:01.971715, Loss 0.005990819 (0.014800844)
Epoch 001, Step 003650/2408065, Time elapsed 0:25:23.111138, Loss 0.004826019 (0.014658379)
Epoch 001, Step 003650/2408065, Time elapsed 0:25:23.111138, Loss 0.004826019 (0.014658379)
Epoch 001, Step 003700/2408065, Time elapsed 0:25:44.694289, Loss 0.004160319 (0.014524847)
Epoch 001, Step 003700/2408065, Time elapsed 0:25:44.694289, Loss 0.004160319 (0.014524847)
Epoch 001, Step 003750/2408065, Time elapsed 0:26:05.360665, Loss 0.004087150 (0.014390618)
Epoch 001, Step 003750/2408065, Time elapsed 0:26:05.360665, Loss 0.004087150 (0.014390618)
Epoch 001, Step 003800/2408065, Time elapsed 0:26:26.160635, Loss 0.004982907 (0.014260728)
Epoch 001, Step 003800/2408065, Time elapsed 0:26:26.160635, Loss 0.004982907 (0.014260728)
Epoch 001, Step 003850/2408065, Time elapsed 0:26:46.801146, Loss 0.005167524 (0.014131218)
Epoch 001, Step 003850/2408065, Time elapsed 0:26:46.801146, Loss 0.005167524 (0.014131218)
Epoch 001, Step 003900/2408065, Time elapsed 0:27:07.787165, Loss 0.004350616 (0.014009791)
Epoch 001, Step 003900/2408065, Time elapsed 0:27:07.787165, Loss 0.004350616 (0.014009791)
Epoch 001, Step 003950/2408065, Time elapsed 0:27:28.240392, Loss 0.004543182 (0.013890984)
Epoch 001, Step 003950/2408065, Time elapsed 0:27:28.240392, Loss 0.004543182 (0.013890984)
Epoch 001, Step 004000/2408065, Time elapsed 0:27:49.121766, Loss 0.005830427 (0.013770518)
Epoch 001, Step 004000/2408065, Time elapsed 0:27:49.121766, Loss 0.005830427 (0.013770518)
Epoch 001, Step 004050/2408065, Time elapsed 0:28:10.015137, Loss 0.003253171 (0.013652544)
Epoch 001, Step 004050/2408065, Time elapsed 0:28:10.015137, Loss 0.003253171 (0.013652544)
Epoch 001, Step 004100/2408065, Time elapsed 0:28:31.509130, Loss 0.005345207 (0.013543659)
Epoch 001, Step 004100/2408065, Time elapsed 0:28:31.509130, Loss 0.005345207 (0.013543659)
Epoch 001, Step 004150/2408065, Time elapsed 0:28:52.072922, Loss 0.002782791 (0.013432504)
Epoch 001, Step 004150/2408065, Time elapsed 0:28:52.072922, Loss 0.002782791 (0.013432504)
Epoch 001, Step 004200/2408065, Time elapsed 0:29:13.259236, Loss 0.004176225 (0.013320966)
Epoch 001, Step 004200/2408065, Time elapsed 0:29:13.259236, Loss 0.004176225 (0.013320966)
Epoch 001, Step 004250/2408065, Time elapsed 0:29:33.399481, Loss 0.004008564 (0.013210749)
Epoch 001, Step 004250/2408065, Time elapsed 0:29:33.399481, Loss 0.004008564 (0.013210749)
Epoch 001, Step 004300/2408065, Time elapsed 0:29:54.986131, Loss 0.003122817 (0.013105546)
Epoch 001, Step 004300/2408065, Time elapsed 0:29:54.986131, Loss 0.003122817 (0.013105546)
Epoch 001, Step 004350/2408065, Time elapsed 0:30:15.176520, Loss 0.002809154 (0.012989558)
Epoch 001, Step 004350/2408065, Time elapsed 0:30:15.176520, Loss 0.002809154 (0.012989558)
Epoch 001, Step 004400/2408065, Time elapsed 0:30:35.490315, Loss 0.003753547 (0.012877179)
Epoch 001, Step 004400/2408065, Time elapsed 0:30:35.490315, Loss 0.003753547 (0.012877179)
Epoch 001, Step 004450/2408065, Time elapsed 0:30:56.094315, Loss 0.002811964 (0.012765183)
Epoch 001, Step 004450/2408065, Time elapsed 0:30:56.094315, Loss 0.002811964 (0.012765183)
Epoch 001, Step 004500/2408065, Time elapsed 0:31:16.448067, Loss 0.002706881 (0.012654325)
Epoch 001, Step 004500/2408065, Time elapsed 0:31:16.448067, Loss 0.002706881 (0.012654325)
Epoch 001, Step 004550/2408065, Time elapsed 0:31:37.103141, Loss 0.002750542 (0.012546243)
Epoch 001, Step 004550/2408065, Time elapsed 0:31:37.103141, Loss 0.002750542 (0.012546243)
Epoch 001, Step 004600/2408065, Time elapsed 0:31:58.075896, Loss 0.002427871 (0.012442302)
Epoch 001, Step 004600/2408065, Time elapsed 0:31:58.075896, Loss 0.002427871 (0.012442302)
Epoch 001, Step 004650/2408065, Time elapsed 0:32:18.353771, Loss 0.002519920 (0.012340958)
Epoch 001, Step 004650/2408065, Time elapsed 0:32:18.353771, Loss 0.002519920 (0.012340958)
Epoch 001, Step 004700/2408065, Time elapsed 0:32:38.498109, Loss 0.004363829 (0.012239505)
Epoch 001, Step 004700/2408065, Time elapsed 0:32:38.498109, Loss 0.004363829 (0.012239505)
Epoch 001, Step 004750/2408065, Time elapsed 0:32:58.832062, Loss 0.002390980 (0.012140030)
Epoch 001, Step 004750/2408065, Time elapsed 0:32:58.832062, Loss 0.002390980 (0.012140030)
Epoch 001, Step 004800/2408065, Time elapsed 0:33:19.630910, Loss 0.002634865 (0.012043993)
Epoch 001, Step 004800/2408065, Time elapsed 0:33:19.630910, Loss 0.002634865 (0.012043993)
Epoch 001, Step 004850/2408065, Time elapsed 0:33:40.896507, Loss 0.002416140 (0.011949005)
Epoch 001, Step 004850/2408065, Time elapsed 0:33:40.896507, Loss 0.002416140 (0.011949005)
Epoch 001, Step 004900/2408065, Time elapsed 0:34:01.411813, Loss 0.003062789 (0.011854362)
Epoch 001, Step 004900/2408065, Time elapsed 0:34:01.411813, Loss 0.003062789 (0.011854362)
Epoch 001, Step 004950/2408065, Time elapsed 0:34:21.813956, Loss 0.002144255 (0.011761803)
Epoch 001, Step 004950/2408065, Time elapsed 0:34:21.813956, Loss 0.002144255 (0.011761803)
Epoch 001, Step 005000/2408065, Time elapsed 0:34:42.350584, Loss 0.004936243 (0.011673042)
Epoch 001, Step 005000/2408065, Time elapsed 0:34:42.350584, Loss 0.004936243 (0.011673042)
Epoch 001, Step 005050/2408065, Time elapsed 0:35:03.093733, Loss 0.002363335 (0.011585266)
Epoch 001, Step 005050/2408065, Time elapsed 0:35:03.093733, Loss 0.002363335 (0.011585266)
Epoch 001, Step 005100/2408065, Time elapsed 0:35:23.472474, Loss 0.002511073 (0.011500854)
Epoch 001, Step 005100/2408065, Time elapsed 0:35:23.472474, Loss 0.002511073 (0.011500854)
Epoch 001, Step 005150/2408065, Time elapsed 0:35:43.785259, Loss 0.002354091 (0.011415472)
Epoch 001, Step 005150/2408065, Time elapsed 0:35:43.785259, Loss 0.002354091 (0.011415472)
Epoch 001, Step 005200/2408065, Time elapsed 0:36:04.445192, Loss 0.002406865 (0.011331724)
Epoch 001, Step 005200/2408065, Time elapsed 0:36:04.445192, Loss 0.002406865 (0.011331724)
Epoch 001, Step 005250/2408065, Time elapsed 0:36:24.837284, Loss 0.002158550 (0.011250371)
Epoch 001, Step 005250/2408065, Time elapsed 0:36:24.837284, Loss 0.002158550 (0.011250371)
Epoch 001, Step 005300/2408065, Time elapsed 0:36:45.386502, Loss 0.003079868 (0.011175762)
Epoch 001, Step 005300/2408065, Time elapsed 0:36:45.386502, Loss 0.003079868 (0.011175762)
Epoch 001, Step 005350/2408065, Time elapsed 0:37:06.152512, Loss 0.011596033 (0.011209782)
Epoch 001, Step 005350/2408065, Time elapsed 0:37:06.152512, Loss 0.011596033 (0.011209782)
Epoch 001, Step 005400/2408065, Time elapsed 0:37:26.454815, Loss 0.003296579 (0.011150172)
Epoch 001, Step 005400/2408065, Time elapsed 0:37:26.454815, Loss 0.003296579 (0.011150172)
Epoch 001, Step 005450/2408065, Time elapsed 0:37:46.824096, Loss 0.003022759 (0.011075606)
Epoch 001, Step 005450/2408065, Time elapsed 0:37:46.824096, Loss 0.003022759 (0.011075606)
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/005473_000001.pt
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/005473_000001.pt
after all OK
Epoch 002, Step 005500/2408065, Time elapsed 0:38:09.233429, Loss 0.002898243 (0.003713115)
Epoch 002, Step 005500/2408065, Time elapsed 0:38:09.233429, Loss 0.002898243 (0.003713115)
Epoch 002, Step 005550/2408065, Time elapsed 0:38:29.552347, Loss 0.003528737 (0.003865221)
Epoch 002, Step 005550/2408065, Time elapsed 0:38:29.552347, Loss 0.003528737 (0.003865221)
Epoch 002, Step 005600/2408065, Time elapsed 0:38:50.081329, Loss 0.003786910 (0.003806733)
Epoch 002, Step 005600/2408065, Time elapsed 0:38:50.081329, Loss 0.003786910 (0.003806733)
Epoch 002, Step 005650/2408065, Time elapsed 0:39:10.061673, Loss 0.003797452 (0.003788080)
Epoch 002, Step 005650/2408065, Time elapsed 0:39:10.061673, Loss 0.003797452 (0.003788080)
Epoch 002, Step 005700/2408065, Time elapsed 0:39:30.316124, Loss 0.003878762 (0.003855840)
Epoch 002, Step 005700/2408065, Time elapsed 0:39:30.316124, Loss 0.003878762 (0.003855840)
Epoch 002, Step 005750/2408065, Time elapsed 0:39:50.449719, Loss 0.003140136 (0.003862409)
Epoch 002, Step 005750/2408065, Time elapsed 0:39:50.449719, Loss 0.003140136 (0.003862409)
Epoch 002, Step 005800/2408065, Time elapsed 0:40:10.571283, Loss 0.005115340 (0.003850445)
Epoch 002, Step 005800/2408065, Time elapsed 0:40:10.571283, Loss 0.005115340 (0.003850445)
Epoch 002, Step 005850/2408065, Time elapsed 0:40:30.728053, Loss 0.004039256 (0.003834972)
Epoch 002, Step 005850/2408065, Time elapsed 0:40:30.728053, Loss 0.004039256 (0.003834972)
Epoch 002, Step 005900/2408065, Time elapsed 0:40:50.894218, Loss 0.003183189 (0.003793105)
Epoch 002, Step 005900/2408065, Time elapsed 0:40:50.894218, Loss 0.003183189 (0.003793105)
Epoch 002, Step 005950/2408065, Time elapsed 0:41:11.076803, Loss 0.003462257 (0.003770254)
Epoch 002, Step 005950/2408065, Time elapsed 0:41:11.076803, Loss 0.003462257 (0.003770254)
Epoch 002, Step 006000/2408065, Time elapsed 0:41:31.259267, Loss 0.003087237 (0.003743069)
Epoch 002, Step 006000/2408065, Time elapsed 0:41:31.259267, Loss 0.003087237 (0.003743069)
Epoch 002, Step 006050/2408065, Time elapsed 0:41:51.455773, Loss 0.003100179 (0.003693706)
Epoch 002, Step 006050/2408065, Time elapsed 0:41:51.455773, Loss 0.003100179 (0.003693706)
Epoch 002, Step 006100/2408065, Time elapsed 0:42:11.554112, Loss 0.002598306 (0.003665288)
Epoch 002, Step 006100/2408065, Time elapsed 0:42:11.554112, Loss 0.002598306 (0.003665288)
Epoch 002, Step 006150/2408065, Time elapsed 0:42:31.626954, Loss 0.004675533 (0.003665897)
Epoch 002, Step 006150/2408065, Time elapsed 0:42:31.626954, Loss 0.004675533 (0.003665897)
Epoch 002, Step 006200/2408065, Time elapsed 0:42:51.741577, Loss 0.002920316 (0.003709248)
Epoch 002, Step 006200/2408065, Time elapsed 0:42:51.741577, Loss 0.002920316 (0.003709248)
Epoch 002, Step 006250/2408065, Time elapsed 0:43:11.975107, Loss 0.003571721 (0.003703734)
Epoch 002, Step 006250/2408065, Time elapsed 0:43:11.975107, Loss 0.003571721 (0.003703734)
Epoch 002, Step 006300/2408065, Time elapsed 0:43:32.190818, Loss 0.003369772 (0.003695740)
Epoch 002, Step 006300/2408065, Time elapsed 0:43:32.190818, Loss 0.003369772 (0.003695740)
Epoch 002, Step 006350/2408065, Time elapsed 0:43:52.193078, Loss 0.003372771 (0.003774087)
Epoch 002, Step 006350/2408065, Time elapsed 0:43:52.193078, Loss 0.003372771 (0.003774087)
Epoch 002, Step 006400/2408065, Time elapsed 0:44:12.369496, Loss 0.004721673 (0.003778073)
Epoch 002, Step 006400/2408065, Time elapsed 0:44:12.369496, Loss 0.004721673 (0.003778073)
Epoch 002, Step 006450/2408065, Time elapsed 0:44:32.463904, Loss 0.003498727 (0.003786996)
Epoch 002, Step 006450/2408065, Time elapsed 0:44:32.463904, Loss 0.003498727 (0.003786996)
Epoch 002, Step 006500/2408065, Time elapsed 0:44:52.747604, Loss 0.008345820 (0.003798125)
Epoch 002, Step 006500/2408065, Time elapsed 0:44:52.747604, Loss 0.008345820 (0.003798125)
Epoch 002, Step 006550/2408065, Time elapsed 0:45:13.090455, Loss 0.003169647 (0.003800446)
Epoch 002, Step 006550/2408065, Time elapsed 0:45:13.090455, Loss 0.003169647 (0.003800446)
Epoch 002, Step 006600/2408065, Time elapsed 0:45:33.313486, Loss 0.003872546 (0.003788022)
Epoch 002, Step 006600/2408065, Time elapsed 0:45:33.313486, Loss 0.003872546 (0.003788022)
Epoch 002, Step 006650/2408065, Time elapsed 0:45:53.904834, Loss 0.003994094 (0.003774634)
Epoch 002, Step 006650/2408065, Time elapsed 0:45:53.904834, Loss 0.003994094 (0.003774634)
Epoch 002, Step 006700/2408065, Time elapsed 0:46:14.349658, Loss 0.003796941 (0.003779838)
Epoch 002, Step 006700/2408065, Time elapsed 0:46:14.349658, Loss 0.003796941 (0.003779838)
Epoch 002, Step 006750/2408065, Time elapsed 0:46:34.565465, Loss 0.005153440 (0.003805551)
Epoch 002, Step 006750/2408065, Time elapsed 0:46:34.565465, Loss 0.005153440 (0.003805551)
Epoch 002, Step 006800/2408065, Time elapsed 0:46:55.104372, Loss 0.003900809 (0.003805828)
Epoch 002, Step 006800/2408065, Time elapsed 0:46:55.104372, Loss 0.003900809 (0.003805828)
Epoch 002, Step 006850/2408065, Time elapsed 0:47:15.415497, Loss 0.002589302 (0.003795443)
Epoch 002, Step 006850/2408065, Time elapsed 0:47:15.415497, Loss 0.002589302 (0.003795443)
Epoch 002, Step 006900/2408065, Time elapsed 0:47:35.623107, Loss 0.004070018 (0.003812879)
Epoch 002, Step 006900/2408065, Time elapsed 0:47:35.623107, Loss 0.004070018 (0.003812879)
Epoch 002, Step 006950/2408065, Time elapsed 0:47:56.792324, Loss 0.004012797 (0.003826360)
Epoch 002, Step 006950/2408065, Time elapsed 0:47:56.792324, Loss 0.004012797 (0.003826360)
Epoch 002, Step 007000/2408065, Time elapsed 0:48:17.179471, Loss 0.004109378 (0.003832426)
Epoch 002, Step 007000/2408065, Time elapsed 0:48:17.179471, Loss 0.004109378 (0.003832426)
Epoch 002, Step 007050/2408065, Time elapsed 0:48:37.403854, Loss 0.004110194 (0.003820687)
Epoch 002, Step 007050/2408065, Time elapsed 0:48:37.403854, Loss 0.004110194 (0.003820687)
Epoch 002, Step 007100/2408065, Time elapsed 0:48:57.715617, Loss 0.003614591 (0.003818076)
Epoch 002, Step 007100/2408065, Time elapsed 0:48:57.715617, Loss 0.003614591 (0.003818076)
Epoch 002, Step 007150/2408065, Time elapsed 0:49:17.858423, Loss 0.002089518 (0.003806474)
Epoch 002, Step 007150/2408065, Time elapsed 0:49:17.858423, Loss 0.002089518 (0.003806474)
Epoch 002, Step 007200/2408065, Time elapsed 0:49:38.050575, Loss 0.002637427 (0.003799018)
Epoch 002, Step 007200/2408065, Time elapsed 0:49:38.050575, Loss 0.002637427 (0.003799018)
Epoch 002, Step 007250/2408065, Time elapsed 0:49:58.262292, Loss 0.003563802 (0.003811782)
Epoch 002, Step 007250/2408065, Time elapsed 0:49:58.262292, Loss 0.003563802 (0.003811782)
Epoch 002, Step 007300/2408065, Time elapsed 0:50:18.418842, Loss 0.003920798 (0.003802617)
Epoch 002, Step 007300/2408065, Time elapsed 0:50:18.418842, Loss 0.003920798 (0.003802617)
Epoch 002, Step 007350/2408065, Time elapsed 0:50:38.464260, Loss 0.002613422 (0.003790751)
Epoch 002, Step 007350/2408065, Time elapsed 0:50:38.464260, Loss 0.002613422 (0.003790751)
Epoch 002, Step 007400/2408065, Time elapsed 0:50:58.409383, Loss 0.010971156 (0.003803725)
Epoch 002, Step 007400/2408065, Time elapsed 0:50:58.409383, Loss 0.010971156 (0.003803725)
Epoch 002, Step 007450/2408065, Time elapsed 0:51:18.409534, Loss 0.001813867 (0.003770194)
Epoch 002, Step 007450/2408065, Time elapsed 0:51:18.409534, Loss 0.001813867 (0.003770194)
Epoch 002, Step 007500/2408065, Time elapsed 0:51:38.665297, Loss 0.003104547 (0.003738773)
Epoch 002, Step 007500/2408065, Time elapsed 0:51:38.665297, Loss 0.003104547 (0.003738773)
Epoch 002, Step 007550/2408065, Time elapsed 0:51:59.128851, Loss 0.002575670 (0.003717791)
Epoch 002, Step 007550/2408065, Time elapsed 0:51:59.128851, Loss 0.002575670 (0.003717791)
Epoch 002, Step 007600/2408065, Time elapsed 0:52:19.506645, Loss 0.003833237 (0.003711797)
Epoch 002, Step 007600/2408065, Time elapsed 0:52:19.506645, Loss 0.003833237 (0.003711797)
Epoch 002, Step 007650/2408065, Time elapsed 0:52:39.860444, Loss 0.003591974 (0.003716906)
Epoch 002, Step 007650/2408065, Time elapsed 0:52:39.860444, Loss 0.003591974 (0.003716906)
Epoch 002, Step 007700/2408065, Time elapsed 0:53:00.230502, Loss 0.003625761 (0.003721279)
Epoch 002, Step 007700/2408065, Time elapsed 0:53:00.230502, Loss 0.003625761 (0.003721279)
Epoch 002, Step 007750/2408065, Time elapsed 0:53:20.496045, Loss 0.003562964 (0.003720197)
Epoch 002, Step 007750/2408065, Time elapsed 0:53:20.496045, Loss 0.003562964 (0.003720197)
Epoch 002, Step 007800/2408065, Time elapsed 0:53:40.915941, Loss 0.003811610 (0.003742806)
Epoch 002, Step 007800/2408065, Time elapsed 0:53:40.915941, Loss 0.003811610 (0.003742806)
Epoch 002, Step 007850/2408065, Time elapsed 0:54:01.630028, Loss 0.022762500 (0.003757780)
Epoch 002, Step 007850/2408065, Time elapsed 0:54:01.630028, Loss 0.022762500 (0.003757780)
Epoch 002, Step 007900/2408065, Time elapsed 0:54:22.259366, Loss 0.004449159 (0.003791673)
Epoch 002, Step 007900/2408065, Time elapsed 0:54:22.259366, Loss 0.004449159 (0.003791673)
Epoch 002, Step 007950/2408065, Time elapsed 0:54:42.767251, Loss 0.006747810 (0.003806583)
Epoch 002, Step 007950/2408065, Time elapsed 0:54:42.767251, Loss 0.006747810 (0.003806583)
Epoch 002, Step 008000/2408065, Time elapsed 0:55:03.568072, Loss 0.002338218 (0.003816738)
Epoch 002, Step 008000/2408065, Time elapsed 0:55:03.568072, Loss 0.002338218 (0.003816738)
Epoch 002, Step 008050/2408065, Time elapsed 0:55:24.042494, Loss 0.003726847 (0.003839874)
Epoch 002, Step 008050/2408065, Time elapsed 0:55:24.042494, Loss 0.003726847 (0.003839874)
Epoch 002, Step 008100/2408065, Time elapsed 0:55:44.701749, Loss 0.002087785 (0.003828188)
Epoch 002, Step 008100/2408065, Time elapsed 0:55:44.701749, Loss 0.002087785 (0.003828188)
Epoch 002, Step 008150/2408065, Time elapsed 0:56:05.283543, Loss 0.001829480 (0.003799360)
Epoch 002, Step 008150/2408065, Time elapsed 0:56:05.283543, Loss 0.001829480 (0.003799360)
Epoch 002, Step 008200/2408065, Time elapsed 0:56:25.905537, Loss 0.001821036 (0.003782865)
Epoch 002, Step 008200/2408065, Time elapsed 0:56:25.905537, Loss 0.001821036 (0.003782865)
Epoch 002, Step 008250/2408065, Time elapsed 0:56:46.434268, Loss 0.003317285 (0.003762819)
Epoch 002, Step 008250/2408065, Time elapsed 0:56:46.434268, Loss 0.003317285 (0.003762819)
Epoch 002, Step 008300/2408065, Time elapsed 0:57:06.787827, Loss 0.002431985 (0.003740509)
Epoch 002, Step 008300/2408065, Time elapsed 0:57:06.787827, Loss 0.002431985 (0.003740509)
Epoch 002, Step 008350/2408065, Time elapsed 0:57:27.338706, Loss 0.004955740 (0.003721256)
Epoch 002, Step 008350/2408065, Time elapsed 0:57:27.338706, Loss 0.004955740 (0.003721256)
Epoch 002, Step 008400/2408065, Time elapsed 0:57:49.385398, Loss 0.002407609 (0.003697007)
Epoch 002, Step 008400/2408065, Time elapsed 0:57:49.385398, Loss 0.002407609 (0.003697007)
Epoch 002, Step 008450/2408065, Time elapsed 0:58:11.243396, Loss 0.001924696 (0.003678451)
Epoch 002, Step 008450/2408065, Time elapsed 0:58:11.243396, Loss 0.001924696 (0.003678451)
Epoch 002, Step 008500/2408065, Time elapsed 0:58:31.902142, Loss 0.002114938 (0.003657764)
Epoch 002, Step 008500/2408065, Time elapsed 0:58:31.902142, Loss 0.002114938 (0.003657764)
Epoch 002, Step 008550/2408065, Time elapsed 0:58:53.026633, Loss 0.002752733 (0.003644284)
Epoch 002, Step 008550/2408065, Time elapsed 0:58:53.026633, Loss 0.002752733 (0.003644284)
Epoch 002, Step 008600/2408065, Time elapsed 0:59:13.494939, Loss 0.002848089 (0.003633648)
Epoch 002, Step 008600/2408065, Time elapsed 0:59:13.494939, Loss 0.002848089 (0.003633648)
Epoch 002, Step 008650/2408065, Time elapsed 0:59:33.619150, Loss 0.003950575 (0.003621115)
Epoch 002, Step 008650/2408065, Time elapsed 0:59:33.619150, Loss 0.003950575 (0.003621115)
Epoch 002, Step 008700/2408065, Time elapsed 0:59:53.863632, Loss 0.002175361 (0.003604584)
Epoch 002, Step 008700/2408065, Time elapsed 0:59:53.863632, Loss 0.002175361 (0.003604584)
Epoch 002, Step 008750/2408065, Time elapsed 1:00:14.419479, Loss 0.004217484 (0.003602610)
Epoch 002, Step 008750/2408065, Time elapsed 1:00:14.419479, Loss 0.004217484 (0.003602610)
Epoch 002, Step 008800/2408065, Time elapsed 1:00:35.045430, Loss 0.003636494 (0.003602698)
Epoch 002, Step 008800/2408065, Time elapsed 1:00:35.045430, Loss 0.003636494 (0.003602698)
Epoch 002, Step 008850/2408065, Time elapsed 1:00:55.505790, Loss 0.003655356 (0.003599145)
Epoch 002, Step 008850/2408065, Time elapsed 1:00:55.505790, Loss 0.003655356 (0.003599145)
Epoch 002, Step 008900/2408065, Time elapsed 1:01:16.384331, Loss 0.004282145 (0.003603288)
Epoch 002, Step 008900/2408065, Time elapsed 1:01:16.384331, Loss 0.004282145 (0.003603288)
Epoch 002, Step 008950/2408065, Time elapsed 1:01:36.889317, Loss 0.003123203 (0.003596923)
Epoch 002, Step 008950/2408065, Time elapsed 1:01:36.889317, Loss 0.003123203 (0.003596923)
Epoch 002, Step 009000/2408065, Time elapsed 1:01:57.325517, Loss 0.002750176 (0.003602897)
Epoch 002, Step 009000/2408065, Time elapsed 1:01:57.325517, Loss 0.002750176 (0.003602897)
Epoch 002, Step 009050/2408065, Time elapsed 1:02:18.100104, Loss 0.003242864 (0.003604451)
Epoch 002, Step 009050/2408065, Time elapsed 1:02:18.100104, Loss 0.003242864 (0.003604451)
Epoch 002, Step 009100/2408065, Time elapsed 1:02:38.560448, Loss 0.003581637 (0.003601511)
Epoch 002, Step 009100/2408065, Time elapsed 1:02:38.560448, Loss 0.003581637 (0.003601511)
Epoch 002, Step 009150/2408065, Time elapsed 1:02:58.825593, Loss 0.003244736 (0.003598275)
Epoch 002, Step 009150/2408065, Time elapsed 1:02:58.825593, Loss 0.003244736 (0.003598275)
Epoch 002, Step 009200/2408065, Time elapsed 1:03:19.023312, Loss 0.002515999 (0.003595071)
Epoch 002, Step 009200/2408065, Time elapsed 1:03:19.023312, Loss 0.002515999 (0.003595071)
Epoch 002, Step 009250/2408065, Time elapsed 1:03:39.461156, Loss 0.003117713 (0.003596975)
Epoch 002, Step 009250/2408065, Time elapsed 1:03:39.461156, Loss 0.003117713 (0.003596975)
Epoch 002, Step 009300/2408065, Time elapsed 1:03:59.584726, Loss 0.003694132 (0.003595946)
Epoch 002, Step 009300/2408065, Time elapsed 1:03:59.584726, Loss 0.003694132 (0.003595946)
Epoch 002, Step 009350/2408065, Time elapsed 1:04:19.963282, Loss 0.002934971 (0.003593263)
Epoch 002, Step 009350/2408065, Time elapsed 1:04:19.963282, Loss 0.002934971 (0.003593263)
Epoch 002, Step 009400/2408065, Time elapsed 1:04:40.155069, Loss 0.002970011 (0.003590342)
Epoch 002, Step 009400/2408065, Time elapsed 1:04:40.155069, Loss 0.002970011 (0.003590342)
Epoch 002, Step 009450/2408065, Time elapsed 1:05:00.848113, Loss 0.003763099 (0.003588171)
Epoch 002, Step 009450/2408065, Time elapsed 1:05:00.848113, Loss 0.003763099 (0.003588171)
Epoch 002, Step 009500/2408065, Time elapsed 1:05:21.522964, Loss 0.002722980 (0.003584644)
Epoch 002, Step 009500/2408065, Time elapsed 1:05:21.522964, Loss 0.002722980 (0.003584644)
Epoch 002, Step 009550/2408065, Time elapsed 1:05:43.409225, Loss 0.003727433 (0.003583886)
Epoch 002, Step 009550/2408065, Time elapsed 1:05:43.409225, Loss 0.003727433 (0.003583886)
Epoch 002, Step 009600/2408065, Time elapsed 1:06:04.416323, Loss 0.002861731 (0.003584056)
Epoch 002, Step 009600/2408065, Time elapsed 1:06:04.416323, Loss 0.002861731 (0.003584056)
Epoch 002, Step 009650/2408065, Time elapsed 1:06:25.293055, Loss 0.004246962 (0.003580383)
Epoch 002, Step 009650/2408065, Time elapsed 1:06:25.293055, Loss 0.004246962 (0.003580383)
Epoch 002, Step 009700/2408065, Time elapsed 1:06:45.515644, Loss 0.003891953 (0.003580553)
Epoch 002, Step 009700/2408065, Time elapsed 1:06:45.515644, Loss 0.003891953 (0.003580553)
Epoch 002, Step 009750/2408065, Time elapsed 1:07:05.710584, Loss 0.002587516 (0.003577131)
Epoch 002, Step 009750/2408065, Time elapsed 1:07:05.710584, Loss 0.002587516 (0.003577131)
Epoch 002, Step 009800/2408065, Time elapsed 1:07:27.539514, Loss 0.002069730 (0.003568279)
Epoch 002, Step 009800/2408065, Time elapsed 1:07:27.539514, Loss 0.002069730 (0.003568279)
Epoch 002, Step 009850/2408065, Time elapsed 1:07:49.573892, Loss 0.002239864 (0.003554119)
Epoch 002, Step 009850/2408065, Time elapsed 1:07:49.573892, Loss 0.002239864 (0.003554119)
Epoch 002, Step 009900/2408065, Time elapsed 1:08:11.599566, Loss 0.002624252 (0.003541104)
Epoch 002, Step 009900/2408065, Time elapsed 1:08:11.599566, Loss 0.002624252 (0.003541104)
Epoch 002, Step 009950/2408065, Time elapsed 1:08:33.574251, Loss 0.002067304 (0.003527277)
Epoch 002, Step 009950/2408065, Time elapsed 1:08:33.574251, Loss 0.002067304 (0.003527277)
Epoch 002, Step 010000/2408065, Time elapsed 1:08:54.024653, Loss 0.001982192 (0.003513386)
Epoch 002, Step 010000/2408065, Time elapsed 1:08:54.024653, Loss 0.001982192 (0.003513386)
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/010000_000002.pt
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/010000_000002.pt
Epoch 002, Step 010050/2408065, Time elapsed 1:09:14.373456, Loss 0.002367090 (0.003500662)
Epoch 002, Step 010050/2408065, Time elapsed 1:09:14.373456, Loss 0.002367090 (0.003500662)
Epoch 002, Step 010100/2408065, Time elapsed 1:09:34.526558, Loss 0.002591967 (0.003490606)
Epoch 002, Step 010100/2408065, Time elapsed 1:09:34.526558, Loss 0.002591967 (0.003490606)
Epoch 002, Step 010150/2408065, Time elapsed 1:09:55.115663, Loss 0.002361363 (0.003478801)
Epoch 002, Step 010150/2408065, Time elapsed 1:09:55.115663, Loss 0.002361363 (0.003478801)
Epoch 002, Step 010200/2408065, Time elapsed 1:10:15.941433, Loss 0.002416938 (0.003467698)
Epoch 002, Step 010200/2408065, Time elapsed 1:10:15.941433, Loss 0.002416938 (0.003467698)
Epoch 002, Step 010250/2408065, Time elapsed 1:10:36.081470, Loss 0.002413074 (0.003456840)
Epoch 002, Step 010250/2408065, Time elapsed 1:10:36.081470, Loss 0.002413074 (0.003456840)
Epoch 002, Step 010300/2408065, Time elapsed 1:10:56.512119, Loss 0.002101978 (0.003446740)
Epoch 002, Step 010300/2408065, Time elapsed 1:10:56.512119, Loss 0.002101978 (0.003446740)
Epoch 002, Step 010350/2408065, Time elapsed 1:11:16.730694, Loss 0.002110656 (0.003435110)
Epoch 002, Step 010350/2408065, Time elapsed 1:11:16.730694, Loss 0.002110656 (0.003435110)
Epoch 002, Step 010400/2408065, Time elapsed 1:11:37.407440, Loss 0.002006283 (0.003423899)
Epoch 002, Step 010400/2408065, Time elapsed 1:11:37.407440, Loss 0.002006283 (0.003423899)
Epoch 002, Step 010450/2408065, Time elapsed 1:11:57.580773, Loss 0.002286774 (0.003412789)
Epoch 002, Step 010450/2408065, Time elapsed 1:11:57.580773, Loss 0.002286774 (0.003412789)
Epoch 002, Step 010500/2408065, Time elapsed 1:12:17.891549, Loss 0.002370753 (0.003401814)
Epoch 002, Step 010500/2408065, Time elapsed 1:12:17.891549, Loss 0.002370753 (0.003401814)
Epoch 002, Step 010550/2408065, Time elapsed 1:12:37.958339, Loss 0.002336892 (0.003391445)
Epoch 002, Step 010550/2408065, Time elapsed 1:12:37.958339, Loss 0.002336892 (0.003391445)
Epoch 002, Step 010600/2408065, Time elapsed 1:12:58.177387, Loss 0.002275774 (0.003382548)
Epoch 002, Step 010600/2408065, Time elapsed 1:12:58.177387, Loss 0.002275774 (0.003382548)
Epoch 002, Step 010650/2408065, Time elapsed 1:13:18.662631, Loss 0.003950683 (0.003372314)
Epoch 002, Step 010650/2408065, Time elapsed 1:13:18.662631, Loss 0.003950683 (0.003372314)
Epoch 002, Step 010700/2408065, Time elapsed 1:13:39.234586, Loss 0.002346890 (0.003363517)
Epoch 002, Step 010700/2408065, Time elapsed 1:13:39.234586, Loss 0.002346890 (0.003363517)
Epoch 002, Step 010750/2408065, Time elapsed 1:13:59.475792, Loss 0.002008452 (0.003356894)
Epoch 002, Step 010750/2408065, Time elapsed 1:13:59.475792, Loss 0.002008452 (0.003356894)
Epoch 002, Step 010800/2408065, Time elapsed 1:14:19.627239, Loss 0.002654959 (0.003365318)
Epoch 002, Step 010800/2408065, Time elapsed 1:14:19.627239, Loss 0.002654959 (0.003365318)
Epoch 002, Step 010850/2408065, Time elapsed 1:14:39.825457, Loss 0.002105004 (0.003361585)
Epoch 002, Step 010850/2408065, Time elapsed 1:14:39.825457, Loss 0.002105004 (0.003361585)
Epoch 002, Step 010900/2408065, Time elapsed 1:15:00.248735, Loss 0.002344642 (0.003358147)
Epoch 002, Step 010900/2408065, Time elapsed 1:15:00.248735, Loss 0.002344642 (0.003358147)
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/010946_000002.pt
Dumping to checkpoint file: /mnt/c/Users/nocer/Desktop/Thesis/p2m/pixel2mesh_ga/checkpoints/010946_000002.pt
after all OK
Epoch 003, Step 010950/2408065, Time elapsed 1:15:21.347405, Loss 0.004439003 (0.003887454)
Epoch 003, Step 010950/2408065, Time elapsed 1:15:21.347405, Loss 0.004439003 (0.003887454)
Epoch 003, Step 011000/2408065, Time elapsed 1:15:41.700390, Loss 0.003312602 (0.003426264)
Epoch 003, Step 011000/2408065, Time elapsed 1:15:41.700390, Loss 0.003312602 (0.003426264)
Epoch 003, Step 011050/2408065, Time elapsed 1:16:02.160702, Loss 0.003157673 (0.003357208)
Epoch 003, Step 011050/2408065, Time elapsed 1:16:02.160702, Loss 0.003157673 (0.003357208)
Epoch 003, Step 011100/2408065, Time elapsed 1:16:22.262408, Loss 0.002342160 (0.003287940)
Epoch 003, Step 011100/2408065, Time elapsed 1:16:22.262408, Loss 0.002342160 (0.003287940)
Epoch 003, Step 011150/2408065, Time elapsed 1:16:43.192593, Loss 0.003071224 (0.003339283)
Epoch 003, Step 011150/2408065, Time elapsed 1:16:43.192593, Loss 0.003071224 (0.003339283)
Epoch 003, Step 011200/2408065, Time elapsed 1:17:03.798234, Loss 0.003415108 (0.003341667)
Epoch 003, Step 011200/2408065, Time elapsed 1:17:03.798234, Loss 0.003415108 (0.003341667)
Epoch 003, Step 011250/2408065, Time elapsed 1:17:24.098201, Loss 0.003068782 (0.003324876)
Epoch 003, Step 011250/2408065, Time elapsed 1:17:24.098201, Loss 0.003068782 (0.003324876)
Epoch 003, Step 011300/2408065, Time elapsed 1:17:44.549288, Loss 0.003242220 (0.003307198)
Epoch 003, Step 011300/2408065, Time elapsed 1:17:44.549288, Loss 0.003242220 (0.003307198)
Epoch 003, Step 011350/2408065, Time elapsed 1:18:04.946628, Loss 0.003842020 (0.003267811)
Epoch 003, Step 011350/2408065, Time elapsed 1:18:04.946628, Loss 0.003842020 (0.003267811)
Epoch 003, Step 011400/2408065, Time elapsed 1:18:25.455012, Loss 0.003730670 (0.003228371)
Epoch 003, Step 011400/2408065, Time elapsed 1:18:25.455012, Loss 0.003730670 (0.003228371)
Epoch 003, Step 011450/2408065, Time elapsed 1:18:46.121964, Loss 0.002351199 (0.003204354)
Epoch 003, Step 011450/2408065, Time elapsed 1:18:46.121964, Loss 0.002351199 (0.003204354)
Epoch 003, Step 011500/2408065, Time elapsed 1:19:06.586130, Loss 0.003110154 (0.003164850)
Epoch 003, Step 011500/2408065, Time elapsed 1:19:06.586130, Loss 0.003110154 (0.003164850)
Epoch 003, Step 011550/2408065, Time elapsed 1:19:27.001442, Loss 0.002177040 (0.003151334)
Epoch 003, Step 011550/2408065, Time elapsed 1:19:27.001442, Loss 0.002177040 (0.003151334)
Epoch 003, Step 011600/2408065, Time elapsed 1:19:47.387913, Loss 0.003167741 (0.003124847)
Epoch 003, Step 011600/2408065, Time elapsed 1:19:47.387913, Loss 0.003167741 (0.003124847)
Epoch 003, Step 011650/2408065, Time elapsed 1:20:07.962217, Loss 0.003141571 (0.003169952)
Epoch 003, Step 011650/2408065, Time elapsed 1:20:07.962217, Loss 0.003141571 (0.003169952)
Epoch 003, Step 011700/2408065, Time elapsed 1:20:28.651448, Loss 0.002618184 (0.003161858)
Epoch 003, Step 011700/2408065, Time elapsed 1:20:28.651448, Loss 0.002618184 (0.003161858)
Epoch 003, Step 011750/2408065, Time elapsed 1:20:49.030206, Loss 0.002474104 (0.003159470)
Epoch 003, Step 011750/2408065, Time elapsed 1:20:49.030206, Loss 0.002474104 (0.003159470)
Epoch 003, Step 011800/2408065, Time elapsed 1:21:09.350160, Loss 0.004021829 (0.003198757)
Epoch 003, Step 011800/2408065, Time elapsed 1:21:09.350160, Loss 0.004021829 (0.003198757)
Epoch 003, Step 011850/2408065, Time elapsed 1:21:29.688681, Loss 0.002359040 (0.003224178)
Epoch 003, Step 011850/2408065, Time elapsed 1:21:29.688681, Loss 0.002359040 (0.003224178)
Epoch 003, Step 011900/2408065, Time elapsed 1:21:49.895451, Loss 0.003519481 (0.003237333)
Epoch 003, Step 011900/2408065, Time elapsed 1:21:49.895451, Loss 0.003519481 (0.003237333)
Epoch 003, Step 011950/2408065, Time elapsed 1:22:09.782181, Loss 0.002730616 (0.003243419)
Epoch 003, Step 011950/2408065, Time elapsed 1:22:09.782181, Loss 0.002730616 (0.003243419)
Epoch 003, Step 012000/2408065, Time elapsed 1:22:29.779902, Loss 0.002603600 (0.003243563)
Epoch 003, Step 012000/2408065, Time elapsed 1:22:29.779902, Loss 0.002603600 (0.003243563)
Epoch 003, Step 012050/2408065, Time elapsed 1:22:49.860633, Loss 0.003399743 (0.003248630)
Epoch 003, Step 012050/2408065, Time elapsed 1:22:49.860633, Loss 0.003399743 (0.003248630)
Epoch 003, Step 012100/2408065, Time elapsed 1:23:10.568224, Loss 0.003202444 (0.003243063)
Epoch 003, Step 012100/2408065, Time elapsed 1:23:10.568224, Loss 0.003202444 (0.003243063)
Epoch 003, Step 012150/2408065, Time elapsed 1:23:30.897387, Loss 0.004795430 (0.003247258)
Epoch 003, Step 012150/2408065, Time elapsed 1:23:30.897387, Loss 0.004795430 (0.003247258)
Epoch 003, Step 012200/2408065, Time elapsed 1:23:51.181941, Loss 0.002865681 (0.003250291)
Epoch 003, Step 012200/2408065, Time elapsed 1:23:51.181941, Loss 0.002865681 (0.003250291)
Epoch 003, Step 012250/2408065, Time elapsed 1:24:11.546991, Loss 0.002787955 (0.003264898)
Epoch 003, Step 012250/2408065, Time elapsed 1:24:11.546991, Loss 0.002787955 (0.003264898)
Epoch 003, Step 012300/2408065, Time elapsed 1:24:31.717463, Loss 0.004644349 (0.003264356)
Epoch 003, Step 012300/2408065, Time elapsed 1:24:31.717463, Loss 0.004644349 (0.003264356)
Epoch 003, Step 012350/2408065, Time elapsed 1:24:51.886656, Loss 0.003607116 (0.003265942)
Epoch 003, Step 012350/2408065, Time elapsed 1:24:51.886656, Loss 0.003607116 (0.003265942)
Epoch 003, Step 012400/2408065, Time elapsed 1:25:11.876461, Loss 0.005205806 (0.003268219)
Epoch 003, Step 012400/2408065, Time elapsed 1:25:11.876461, Loss 0.005205806 (0.003268219)
Epoch 003, Step 012450/2408065, Time elapsed 1:25:31.941428, Loss 0.003440549 (0.003276078)
Epoch 003, Step 012450/2408065, Time elapsed 1:25:31.941428, Loss 0.003440549 (0.003276078)
Epoch 003, Step 012500/2408065, Time elapsed 1:25:52.124590, Loss 0.003332894 (0.003276384)
Epoch 003, Step 012500/2408065, Time elapsed 1:25:52.124590, Loss 0.003332894 (0.003276384)
Epoch 003, Step 012550/2408065, Time elapsed 1:26:12.351159, Loss 0.003261087 (0.003276379)
Epoch 003, Step 012550/2408065, Time elapsed 1:26:12.351159, Loss 0.003261087 (0.003276379)
Epoch 003, Step 012600/2408065, Time elapsed 1:26:32.672745, Loss 0.004780196 (0.003277579)
Epoch 003, Step 012600/2408065, Time elapsed 1:26:32.672745, Loss 0.004780196 (0.003277579)
Epoch 003, Step 012650/2408065, Time elapsed 1:26:53.136849, Loss 0.002298327 (0.003268069)
Epoch 003, Step 012650/2408065, Time elapsed 1:26:53.136849, Loss 0.002298327 (0.003268069)










model n parameters: ORIGINALE + self attention (only init)
Total Parameters: 138.926.087
Model Size: 529.96 MB


9 milioni di differenza aggiungendo o sottranedo la geometric algebra ... 

model n parameters: ORIGINALE
Total Parameters: 129.776.153
Model Size: 495.06 MB
Using GPUs: [0]


Total Parameters: 129.776.153
Model Size: 495.06 MB


model n parameters: MIO 
Total Parameters: 96.800.868
Model Size: 369.27 MB

GA REFINEMENT ->  ga_refinement(
  (gcns): GBottleneck(
    (blocks): Sequential(
      (0): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
      (1): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
      (2): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
      (3): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
      (4): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
      (5): GResBlock(
        (conv1): GConv (192 -> 192)
        (conv2): GConv (192 -> 192)
      )
    )
    (conv1): GConv (4043 -> 192)
    (conv2): GConv (192 -> 192)
  )
  (gconv): GConv (192 -> 3)
  (unpooling): GUnpooling (617 -> 2465)
  (algebra): CliffordAlgebra()
  (self_attention_ga): SelfAttentionGA(
    (algebra): CliffordAlgebra()
    (ga_attention): GeometricProductAttention(
      (algebra): CliffordAlgebra()
      (gp_layer): FullyConnectedSteerableGeometricProductLayer(
        (algebra): CliffordAlgebra()
        (normalization): NormalizationLayer(
          (algebra): CliffordAlgebra()
        )
        (q_prj): MVLinear(
          (algebra): CliffordAlgebra()
        )
        (k_prj): MVLinear(
          (algebra): CliffordAlgebra()
        )
      )
      (att_prj): Linear(in_features=8, out_features=1, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (v_proj): Linear(in_features=8, out_features=8, bias=True)
  )
)





#MODEL MISMATCH DEBUG

Eval missing key
sto qua (DataParallel):
missing key =>  _IncompatibleKeys(missing_keys=['gcns.0.blocks.0.conv1.adj_mat', 'gcns.0.blocks.0.conv2.adj_mat', 'gcns.0.blocks.1.conv1.adj_mat', 'gcns.0.blocks.1.conv2.adj_mat', 'gcns.0.blocks.2.conv1.adj_mat', 'gcns.0.blocks.2.conv2.adj_mat', 'gcns.0.blocks.3.conv1.adj_mat', 'gcns.0.blocks.3.conv2.adj_mat', 'gcns.0.blocks.4.conv1.adj_mat', 'gcns.0.blocks.4.conv2.adj_mat', 'gcns.0.blocks.5.conv1.adj_mat', 'gcns.0.blocks.5.conv2.adj_mat', 'gcns.0.conv1.adj_mat', 'gcns.0.conv2.adj_mat', 'gcns.1.blocks.0.conv1.adj_mat', 'gcns.1.blocks.0.conv2.adj_mat', 'gcns.1.blocks.1.conv1.adj_mat', 'gcns.1.blocks.1.conv2.adj_mat', 'gcns.1.blocks.2.conv1.adj_mat', 'gcns.1.blocks.2.conv2.adj_mat', 'gcns.1.blocks.3.conv1.adj_mat', 'gcns.1.blocks.3.conv2.adj_mat', 'gcns.1.blocks.4.conv1.adj_mat', 'gcns.1.blocks.4.conv2.adj_mat', 'gcns.1.blocks.5.conv1.adj_mat', 'gcns.1.blocks.5.conv2.adj_mat', 'gcns.1.conv1.adj_mat', 'gcns.1.conv2.adj_mat', 'gcns.2.blocks.0.conv1.adj_mat', 'gcns.2.blocks.0.conv2.adj_mat', 'gcns.2.blocks.1.conv1.adj_mat', 'gcns.2.blocks.1.conv2.adj_mat', 'gcns.2.blocks.2.conv1.adj_mat', 'gcns.2.blocks.2.conv2.adj_mat', 'gcns.2.blocks.3.conv1.adj_mat', 'gcns.2.blocks.3.conv2.adj_mat', 'gcns.2.blocks.4.conv1.adj_mat', 'gcns.2.blocks.4.conv2.adj_mat', 'gcns.2.blocks.5.conv1.adj_mat', 'gcns.2.blocks.5.conv2.adj_mat', 'gcns.2.conv1.adj_mat', 'gcns.2.conv2.adj_mat', 'gconv.adj_mat'], unexpected_keys=[])

predict missing key
sto qua ():
missing key =>  _IncompatibleKeys(missing_keys=['gcns.0.blocks.0.conv1.adj_mat', 'gcns.0.blocks.0.conv2.adj_mat', 'gcns.0.blocks.1.conv1.adj_mat', 'gcns.0.blocks.1.conv2.adj_mat', 'gcns.0.blocks.2.conv1.adj_mat', 'gcns.0.blocks.2.conv2.adj_mat', 'gcns.0.blocks.3.conv1.adj_mat', 'gcns.0.blocks.3.conv2.adj_mat', 'gcns.0.blocks.4.conv1.adj_mat', 'gcns.0.blocks.4.conv2.adj_mat', 'gcns.0.blocks.5.conv1.adj_mat', 'gcns.0.blocks.5.conv2.adj_mat', 'gcns.0.conv1.adj_mat', 'gcns.0.conv2.adj_mat', 'gcns.1.blocks.0.conv1.adj_mat', 'gcns.1.blocks.0.conv2.adj_mat', 'gcns.1.blocks.1.conv1.adj_mat', 'gcns.1.blocks.1.conv2.adj_mat', 'gcns.1.blocks.2.conv1.adj_mat', 'gcns.1.blocks.2.conv2.adj_mat', 'gcns.1.blocks.3.conv1.adj_mat', 'gcns.1.blocks.3.conv2.adj_mat', 'gcns.1.blocks.4.conv1.adj_mat', 'gcns.1.blocks.4.conv2.adj_mat', 'gcns.1.blocks.5.conv1.adj_mat', 'gcns.1.blocks.5.conv2.adj_mat', 'gcns.1.conv1.adj_mat', 'gcns.1.conv2.adj_mat', 'gcns.2.blocks.0.conv1.adj_mat', 'gcns.2.blocks.0.conv2.adj_mat', 'gcns.2.blocks.1.conv1.adj_mat', 'gcns.2.blocks.1.conv2.adj_mat', 'gcns.2.blocks.2.conv1.adj_mat', 'gcns.2.blocks.2.conv2.adj_mat', 'gcns.2.blocks.3.conv1.adj_mat', 'gcns.2.blocks.3.conv2.adj_mat', 'gcns.2.blocks.4.conv1.adj_mat', 'gcns.2.blocks.4.conv2.adj_mat', 'gcns.2.blocks.5.conv1.adj_mat', 'gcns.2.blocks.5.conv2.adj_mat', 'gcns.2.conv1.adj_mat', 'gcns.2.conv2.adj_mat', 'gconv.adj_mat'], unexpected_keys=[])



missing keys -> _IncompatibleKeys(missing_keys=[
  'gcns.0.blocks.0.conv1.adj_mat', 'gcns.0.blocks.0.conv2.adj_mat', 'gcns.0.blocks.1.conv1.adj_mat', 'gcns.0.blocks.1.conv2.adj_mat', 'gcns.0.blocks.2.conv1.adj_mat', 'gcns.0.blocks.2.conv2.adj_mat', 'gcns.0.blocks.3.conv1.adj_mat', 'gcns.0.blocks.3.conv2.adj_mat', 'gcns.0.blocks.4.conv1.adj_mat', 'gcns.0.blocks.4.conv2.adj_mat', 'gcns.0.blocks.5.conv1.adj_mat', 'gcns.0.blocks.5.conv2.adj_mat', 'gcns.0.conv1.adj_mat', 'gcns.0.conv2.adj_mat', 'gcns.1.blocks.0.conv1.adj_mat', 'gcns.1.blocks.0.conv2.adj_mat', 'gcns.1.blocks.1.conv1.adj_mat', 'gcns.1.blocks.1.conv2.adj_mat', 'gcns.1.blocks.2.conv1.adj_mat', 'gcns.1.blocks.2.conv2.adj_mat', 'gcns.1.blocks.3.conv1.adj_mat', 'gcns.1.blocks.3.conv2.adj_mat', 'gcns.1.blocks.4.conv1.adj_mat', 'gcns.1.blocks.4.conv2.adj_mat', 'gcns.1.blocks.5.conv1.adj_mat', 'gcns.1.blocks.5.conv2.adj_mat', 'gcns.1.conv1.adj_mat', 'gcns.1.conv2.adj_mat', 'gcns.2.blocks.0.conv1.adj_mat', 'gcns.2.blocks.0.conv2.adj_mat', 'gcns.2.blocks.1.conv1.adj_mat', 'gcns.2.blocks.1.conv2.adj_mat', 'gcns.2.blocks.2.conv1.adj_mat', 'gcns.2.blocks.2.conv2.adj_mat', 'gcns.2.blocks.3.conv1.adj_mat', 'gcns.2.blocks.3.conv2.adj_mat', 'gcns.2.blocks.4.conv1.adj_mat', 'gcns.2.blocks.4.conv2.adj_mat', 'gcns.2.blocks.5.conv1.adj_mat', 'gcns.2.blocks.5.conv2.adj_mat', 'gcns.2.conv1.adj_mat', 'gcns.2.conv2.adj_mat', 'gconv.adj_mat'], unexpected_keys=[])



