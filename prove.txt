

# FILE ONLY FOR TESTING PURPOSE



# visualize the prediction 
python entrypoint_predict.py --name nonso --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar --folder datasets/examples/


python entrypoint_predict.py --options /path/to/yml --checkpoint /path/to/checkpoint --folder /path/to/images



#training all 
python entrypoint_train.py --name nonso --options experiments/default/resnet.yml

#training new model (for fine tunning)
python entrypoint_train_ga.py --name nonso --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar --folder datasets/examples/



#test all

python entrypoint_eval.py --name prova_0 --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar 

python entrypoint_train_ga.py --name prova_1 --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar 


''' RESULT ORIGINAL (RESNET) '''
Test Step 000000/005472 (000000) cd 0.000697 (0.000054), f1_tau 0.391795 (0.030138), f1_2tau 0.590053 (0.045389)
Test Step 000050/005472 (000050) cd 0.000916 (0.000070), f1_tau 0.319681 (0.024591), f1_2tau 0.505144 (0.038857)
Test Step 000100/005472 (000100) cd 0.000895 (0.000069), f1_tau 0.321660 (0.024743), f1_2tau 0.507799 (0.039061)
Test Step 000150/005472 (000150) cd 0.000916 (0.000070), f1_tau 0.320202 (0.024631), f1_2tau 0.503963 (0.038766)
Test Step 000200/005472 (000200) cd 0.000924 (0.000071), f1_tau 0.322334 (0.024795), f1_2tau 0.507239 (0.039018)
Test Step 000250/005472 (000250) cd 0.000931 (0.000072), f1_tau 0.321201 (0.024708), f1_2tau 0.505700 (0.038900)
Test Step 000300/005472 (000300) cd 0.000921 (0.000071), f1_tau 0.321062 (0.024697), f1_2tau 0.505326 (0.038871)
Test Step 000350/005472 (000350) cd 0.000915 (0.000070), f1_tau 0.320822 (0.024679), f1_2tau 0.505116 (0.038855)
Test Step 000400/005472 (000400) cd 0.000911 (0.000127), f1_tau 0.322159 (0.050435), f1_2tau 0.506425 (0.079279)
Test Step 000450/005472 (000450) cd 0.000893 (0.000128), f1_tau 0.322063 (0.049563), f1_2tau 0.508164 (0.079117)
Test Step 000500/005472 (000500) cd 0.000878 (0.000128), f1_tau 0.321384 (0.049304), f1_2tau 0.508894 (0.078866)
Test Step 000550/005472 (000550) cd 0.000864 (0.000127), f1_tau 0.322225 (0.049596), f1_2tau 0.511139 (0.079230)
Test Step 000600/005472 (000600) cd 0.000857 (0.000183), f1_tau 0.322832 (0.083742), f1_2tau 0.512013 (0.125279)
Test Step 000650/005472 (000650) cd 0.000850 (0.000187), f1_tau 0.331063 (0.082602), f1_2tau 0.518382 (0.124788)
Test Step 000700/005472 (000700) cd 0.000863 (0.000196), f1_tau 0.336034 (0.081476), f1_2tau 0.521561 (0.123612)
Test Step 000750/005472 (000750) cd 0.000865 (0.000196), f1_tau 0.341548 (0.081505), f1_2tau 0.525472 (0.123603)
Test Step 000800/005472 (000800) cd 0.000870 (0.000197), f1_tau 0.344795 (0.081051), f1_2tau 0.527352 (0.123142)
Test Step 000850/005472 (000850) cd 0.000875 (0.000277), f1_tau 0.346266 (0.107352), f1_2tau 0.528098 (0.162908)
Test Step 000900/005472 (000900) cd 0.000879 (0.000272), f1_tau 0.345730 (0.107112), f1_2tau 0.527161 (0.162647)
Test Step 000950/005472 (000950) cd 0.000878 (0.000268), f1_tau 0.346327 (0.107658), f1_2tau 0.527791 (0.163407)
Test Step 001000/005472 (001000) cd 0.000878 (0.000268), f1_tau 0.346753 (0.107854), f1_2tau 0.528131 (0.163649)
Test Step 001050/005472 (001050) cd 0.000879 (0.000267), f1_tau 0.347269 (0.108011), f1_2tau 0.528623 (0.163852)
Test Step 001100/005472 (001100) cd 0.000881 (0.000267), f1_tau 0.347606 (0.108073), f1_2tau 0.528813 (0.163904)
Test Step 001150/005472 (001150) cd 0.000878 (0.000266), f1_tau 0.347855 (0.108100), f1_2tau 0.529227 (0.164005)
Test Step 001200/005472 (001200) cd 0.000879 (0.000266), f1_tau 0.348108 (0.108126), f1_2tau 0.529518 (0.164059)
Test Step 001250/005472 (001250) cd 0.000880 (0.000266), f1_tau 0.348180 (0.108110), f1_2tau 0.529518 (0.164040)
Test Step 001300/005472 (001300) cd 0.000885 (0.000267), f1_tau 0.348171 (0.108081), f1_2tau 0.529259 (0.163971)
Test Step 001350/005472 (001350) cd 0.000884 (0.000266), f1_tau 0.348352 (0.108095), f1_2tau 0.529394 (0.163988)
Test Step 001400/005472 (001400) cd 0.000884 (0.000266), f1_tau 0.348514 (0.108106), f1_2tau 0.529460 (0.163991)
Test Step 001450/005472 (001450) cd 0.000885 (0.000266), f1_tau 0.348584 (0.108100), f1_2tau 0.529458 (0.163983)
Test Step 001500/005472 (001500) cd 0.000887 (0.000267), f1_tau 0.348511 (0.108072), f1_2tau 0.529295 (0.163949)
Test Step 001550/005472 (001550) cd 0.000886 (0.000266), f1_tau 0.348517 (0.108060), f1_2tau 0.529336 (0.163950)
Test Step 001600/005472 (001600) cd 0.000887 (0.000266), f1_tau 0.348710 (0.108079), f1_2tau 0.529364 (0.163950)
Test Step 001650/005472 (001650) cd 0.000888 (0.000267), f1_tau 0.348632 (0.108056), f1_2tau 0.529257 (0.163930)
Test Step 001700/005472 (001700) cd 0.000887 (0.000336), f1_tau 0.349632 (0.137344), f1_2tau 0.530143 (0.206413)
Test Step 001750/005472 (001750) cd 0.000890 (0.000339), f1_tau 0.350009 (0.136554), f1_2tau 0.530242 (0.205595)
Test Step 001800/005472 (001800) cd 0.000891 (0.000339), f1_tau 0.350531 (0.136527), f1_2tau 0.530467 (0.205526)
Test Step 001850/005472 (001850) cd 0.000889 (0.000401), f1_tau 0.356245 (0.180758), f1_2tau 0.534698 (0.259130)
Test Step 001900/005472 (001900) cd 0.000890 (0.000406), f1_tau 0.361334 (0.179762), f1_2tau 0.538344 (0.258203)
Test Step 001950/005472 (001950) cd 0.000887 (0.000403), f1_tau 0.366941 (0.180241), f1_2tau 0.542522 (0.258644)
Test Step 002000/005472 (002000) cd 0.000885 (0.000403), f1_tau 0.371846 (0.180147), f1_2tau 0.546129 (0.258577)
Test Step 002050/005472 (002050) cd 0.000882 (0.000402), f1_tau 0.376703 (0.180214), f1_2tau 0.549826 (0.258707)
Test Step 002100/005472 (002100) cd 0.000882 (0.000457), f1_tau 0.381044 (0.210682), f1_2tau 0.552985 (0.304958)
Test Step 002150/005472 (002150) cd 0.000889 (0.000491), f1_tau 0.378738 (0.201837), f1_2tau 0.550791 (0.293935)
Test Step 002200/005472 (002200) cd 0.000892 (0.000488), f1_tau 0.376519 (0.201786), f1_2tau 0.548745 (0.293988)
Test Step 002250/005472 (002250) cd 0.000896 (0.000486), f1_tau 0.374501 (0.201886), f1_2tau 0.546951 (0.294192)
Test Step 002300/005472 (002300) cd 0.000900 (0.000486), f1_tau 0.372673 (0.202027), f1_2tau 0.545188 (0.294253)
Test Step 002350/005472 (002350) cd 0.000910 (0.000592), f1_tau 0.372267 (0.229430), f1_2tau 0.544255 (0.332929)
Test Step 002400/005472 (002400) cd 0.000918 (0.000588), f1_tau 0.372697 (0.230871), f1_2tau 0.544186 (0.334427)
Test Step 002450/005472 (002450) cd 0.000930 (0.000594), f1_tau 0.372576 (0.230659), f1_2tau 0.543545 (0.334181)
Test Step 002500/005472 (002500) cd 0.000939 (0.000593), f1_tau 0.372610 (0.230701), f1_2tau 0.543061 (0.334187)
Test Step 002550/005472 (002550) cd 0.000949 (0.000594), f1_tau 0.372117 (0.230309), f1_2tau 0.542121 (0.333815)
Test Step 002600/005472 (002600) cd 0.000958 (0.000637), f1_tau 0.371997 (0.262802), f1_2tau 0.541676 (0.381437)
Test Step 002650/005472 (002650) cd 0.000950 (0.000638), f1_tau 0.373827 (0.265754), f1_2tau 0.543993 (0.384290)
Test Step 002700/005472 (002700) cd 0.000944 (0.000640), f1_tau 0.374980 (0.264809), f1_2tau 0.545762 (0.383620)
Test Step 002750/005472 (002750) cd 0.000943 (0.000706), f1_tau 0.376705 (0.304239), f1_2tau 0.547313 (0.433763)
Test Step 002800/005472 (002800) cd 0.000939 (0.000704), f1_tau 0.379356 (0.304541), f1_2tau 0.549495 (0.434177)
Test Step 002850/005472 (002850) cd 0.000936 (0.000704), f1_tau 0.381848 (0.304491), f1_2tau 0.551509 (0.434105)
Test Step 002900/005472 (002900) cd 0.000933 (0.000704), f1_tau 0.384461 (0.304731), f1_2tau 0.553649 (0.434321)
Test Step 002950/005472 (002950) cd 0.000930 (0.000705), f1_tau 0.386539 (0.304418), f1_2tau 0.555308 (0.434034)
Test Step 003000/005472 (003000) cd 0.000926 (0.000704), f1_tau 0.388882 (0.304497), f1_2tau 0.557258 (0.434138)
Test Step 003050/005472 (003050) cd 0.000925 (0.000705), f1_tau 0.390753 (0.304268), f1_2tau 0.558700 (0.433892)
Test Step 003100/005472 (003100) cd 0.000924 (0.000706), f1_tau 0.392599 (0.304122), f1_2tau 0.560100 (0.433714)
Test Step 003150/005472 (003150) cd 0.000923 (0.000707), f1_tau 0.394454 (0.304049), f1_2tau 0.561554 (0.433633)
Test Step 003200/005472 (003200) cd 0.000922 (0.000707), f1_tau 0.395983 (0.303852), f1_2tau 0.562693 (0.433430)
Test Step 003250/005472 (003250) cd 0.000919 (0.000772), f1_tau 0.396655 (0.328910), f1_2tau 0.563263 (0.472232)
Test Step 003300/005472 (003300) cd 0.000920 (0.000780), f1_tau 0.395491 (0.328617), f1_2tau 0.562236 (0.471788)
Test Step 003350/005472 (003350) cd 0.000920 (0.000778), f1_tau 0.394662 (0.329205), f1_2tau 0.561568 (0.472409)
Test Step 003400/005472 (003400) cd 0.000921 (0.000779), f1_tau 0.393580 (0.329025), f1_2tau 0.560651 (0.472257)
Test Step 003450/005472 (003450) cd 0.000921 (0.000779), f1_tau 0.392665 (0.329089), f1_2tau 0.559896 (0.472336)
Test Step 003500/005472 (003500) cd 0.000922 (0.000780), f1_tau 0.391728 (0.329082), f1_2tau 0.559084 (0.472307)
Test Step 003550/005472 (003550) cd 0.000924 (0.000781), f1_tau 0.390718 (0.328993), f1_2tau 0.558103 (0.472125)
Test Step 003600/005472 (003600) cd 0.000924 (0.000781), f1_tau 0.389886 (0.329039), f1_2tau 0.557437 (0.472206)
Test Step 003650/005472 (003650) cd 0.000924 (0.000780), f1_tau 0.389006 (0.329028), f1_2tau 0.556689 (0.472201)
Test Step 003700/005472 (003700) cd 0.000924 (0.000780), f1_tau 0.388269 (0.329091), f1_2tau 0.556083 (0.472271)
Test Step 003750/005472 (003750) cd 0.000924 (0.000780), f1_tau 0.387446 (0.329083), f1_2tau 0.555377 (0.472263)
Test Step 003800/005472 (003800) cd 0.000925 (0.000780), f1_tau 0.386765 (0.329138), f1_2tau 0.554742 (0.472284)
Test Step 003850/005472 (003850) cd 0.000925 (0.000780), f1_tau 0.386113 (0.329190), f1_2tau 0.554242 (0.472357)
Test Step 003900/005472 (003900) cd 0.000926 (0.000780), f1_tau 0.385320 (0.329164), f1_2tau 0.553455 (0.472286)
Test Step 003950/005472 (003950) cd 0.000927 (0.000781), f1_tau 0.384450 (0.329100), f1_2tau 0.552647 (0.472207)
Test Step 004000/005472 (004000) cd 0.000926 (0.000780), f1_tau 0.383865 (0.329149), f1_2tau 0.552187 (0.472269)
Test Step 004050/005472 (004050) cd 0.000925 (0.000780), f1_tau 0.383264 (0.329182), f1_2tau 0.551704 (0.472311)
Test Step 004100/005472 (004100) cd 0.000927 (0.000780), f1_tau 0.382693 (0.329216), f1_2tau 0.551207 (0.472339)
Test Step 004150/005472 (004150) cd 0.000928 (0.000780), f1_tau 0.381937 (0.329177), f1_2tau 0.550508 (0.472289)
Test Step 004200/005472 (004200) cd 0.000928 (0.000780), f1_tau 0.381376 (0.329201), f1_2tau 0.550024 (0.472310)
Test Step 004250/005472 (004250) cd 0.000928 (0.000780), f1_tau 0.380702 (0.329182), f1_2tau 0.549453 (0.472298)
Test Step 004300/005472 (004300) cd 0.000928 (0.000825), f1_tau 0.380207 (0.360450), f1_2tau 0.549072 (0.518925)
Test Step 004350/005472 (004350) cd 0.000923 (0.000823), f1_tau 0.380565 (0.360771), f1_2tau 0.549837 (0.519545)
Test Step 004400/005472 (004400) cd 0.000920 (0.000825), f1_tau 0.380798 (0.360425), f1_2tau 0.550441 (0.519140)
Test Step 004450/005472 (004450) cd 0.000916 (0.000824), f1_tau 0.381140 (0.360546), f1_2tau 0.551157 (0.519264)
Test Step 004500/005472 (004500) cd 0.000912 (0.000824), f1_tau 0.381530 (0.360702), f1_2tau 0.551927 (0.519446)
Test Step 004550/005472 (004550) cd 0.000908 (0.000824), f1_tau 0.381777 (0.360615), f1_2tau 0.552536 (0.519360)
Test Step 004600/005472 (004600) cd 0.000904 (0.000824), f1_tau 0.382108 (0.360659), f1_2tau 0.553241 (0.519429)
Test Step 004650/005472 (004650) cd 0.000901 (0.000825), f1_tau 0.382313 (0.360571), f1_2tau 0.553765 (0.519312)
Test Step 004700/005472 (004700) cd 0.000898 (0.000825), f1_tau 0.382548 (0.360535), f1_2tau 0.554324 (0.519265)
Test Step 004750/005472 (004750) cd 0.000894 (0.000825), f1_tau 0.382732 (0.360471), f1_2tau 0.554848 (0.519209)
Test Step 004800/005472 (004800) cd 0.000891 (0.000825), f1_tau 0.382905 (0.360414), f1_2tau 0.555327 (0.519140)
Test Step 004850/005472 (004850) cd 0.000888 (0.000825), f1_tau 0.383131 (0.360405), f1_2tau 0.555838 (0.519111)
Test Step 004900/005472 (004900) cd 0.000885 (0.000825), f1_tau 0.383270 (0.360347), f1_2tau 0.556282 (0.519052)
Test Step 004950/005472 (004950) cd 0.000882 (0.000825), f1_tau 0.383511 (0.360357), f1_2tau 0.556831 (0.519068)
Test Step 005000/005472 (005000) cd 0.000879 (0.000825), f1_tau 0.383769 (0.360379), f1_2tau 0.557394 (0.519095)
Test Step 005050/005472 (005050) cd 0.000876 (0.000825), f1_tau 0.384065 (0.360420), f1_2tau 0.557961 (0.519126)
Test Step 005100/005472 (005100) cd 0.000873 (0.000825), f1_tau 0.384267 (0.360412), f1_2tau 0.558436 (0.519114)
Test Step 005150/005472 (005150) cd 0.000870 (0.000825), f1_tau 0.384467 (0.360407), f1_2tau 0.558892 (0.519099)
Test Step 005200/005472 (005200) cd 0.000868 (0.000825), f1_tau 0.384670 (0.360405), f1_2tau 0.559354 (0.519093)
Test Step 005250/005472 (005250) cd 0.000866 (0.000894), f1_tau 0.384982 (0.393539), f1_2tau 0.559736 (0.564057)
Test Step 005300/005472 (005300) cd 0.000866 (0.000894), f1_tau 0.385602 (0.394631), f1_2tau 0.560181 (0.565283)
Test Step 005350/005472 (005350) cd 0.000867 (0.000894), f1_tau 0.386141 (0.394581), f1_2tau 0.560534 (0.565203)
Test Step 005400/005472 (005400) cd 0.000868 (0.000896), f1_tau 0.386379 (0.393852), f1_2tau 0.560580 (0.564439)
Test Step 005450/005472 (005450) cd 0.000867 (0.000894), f1_tau 0.386974 (0.394140), f1_2tau 0.560987 (0.564710)
Test [005473] cd: 0.000895
Test [005473] f1_tau: 0.393849
Test [005473] f1_2tau: 0.564412






















okay bene ora vorrei modificare questo codice per migliorarlo utilizzando la geometric algebra hai qualche idea da propormi? Che ne pensi modificare i livelli GCN in modo che siano E(3)-equivarianti, introducendo meccanismi di attenzione basati su GA. I GA consentono di gestire rotazioni e traslazioni in modo più naturale, quindi si potrebbero costruire livelli GCN che siano intrinsecamente equivarianti a tali trasformazioni, migliorando la capacità della rete di generalizzare tra diverse pose e scale dell'oggetto nell'immagine di input.
Implementare i livelli di attenzione utilizzando prodotti geometrici per calcolare i punteggi di attenzione tra i vertici della mesh, considerando sia le posizioni che le relazioni geometriche (come le direzioni dei bordi o le normali della superficie) rappresentate come multivettori.  Mi puoi aiutarlo a fare? Ho bisogno anche di capire se cio' funzionerebbe oppure no!
tuttavia sentiti libero di espormi le critiche di questo approccio e nel caso migliorarlo.

import torch
import torch.nn as nn
import torch.nn.functional as F

import sys # for stopping whenever I want!

from models.backbones import get_backbone
from models.layers.gbottleneck import GBottleneck
from models.layers.gconv import GConv
from models.layers.gpooling import GUnpooling
from models.layers.gprojection import GProjection


class P2MModel(nn.Module):

    def __init__(self, options, ellipsoid, camera_f, camera_c, mesh_pos):
        super(P2MModel, self).__init__()

        self.hidden_dim = options.hidden_dim
        self.coord_dim = options.coord_dim
        self.last_hidden_dim = options.last_hidden_dim
        self.init_pts = nn.Parameter(ellipsoid.coord, requires_grad=False)
        self.gconv_activation = options.gconv_activation

        self.nn_encoder, self.nn_decoder = get_backbone(options)
        self.features_dim = self.nn_encoder.features_dim + self.coord_dim

        self.gcns = nn.ModuleList([
            GBottleneck(6, self.features_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[0], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[1], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.last_hidden_dim,
                        ellipsoid.adj_mat[2], activation=self.gconv_activation)
        ])

        self.unpooling = nn.ModuleList([
            GUnpooling(ellipsoid.unpool_idx[0]),
            GUnpooling(ellipsoid.unpool_idx[1])
        ])

        # if options.align_with_tensorflow:
        #     self.projection = GProjection
        # else:
        #     self.projection = GProjection
        self.projection = GProjection(mesh_pos, camera_f, camera_c, bound=options.z_threshold,
                                      tensorflow_compatible=options.align_with_tensorflow)

        self.gconv = GConv(in_features=self.last_hidden_dim, out_features=self.coord_dim,
                           adj_mat=ellipsoid.adj_mat[2])

    def forward(self, img):
        batch_size = img.size(0)
        img_feats = self.nn_encoder(img)
        img_shape = self.projection.image_feature_shape(img)

        init_pts = self.init_pts.data.unsqueeze(0).expand(batch_size, -1, -1)


        # GCN Block 1
        x = self.projection(img_shape, img_feats, init_pts)
  
        x1, x_hidden = self.gcns[0](x)
        # x1 -> new coordinate of vertices => ex: torch.Size([8, 156, 3])
        # x_hidden -> feature learned during gcn => ex:  torch.Size([8, 156, 192])
    

        # before deformation 2
        x1_up = self.unpooling[0](x1)

        # GCN Block 2
        x = self.projection(img_shape, img_feats, x1)
       
        x = self.unpooling[0](torch.cat([x, x_hidden], 2))
  
        # after deformation 2
        x2, x_hidden = self.gcns[1](x)


        # before deformation 3
        x2_up = self.unpooling[1](x2)
 

        # GCN Block 3
        x = self.projection(img_shape, img_feats, x2)

        x = self.unpooling[1](torch.cat([x, x_hidden], 2))
   
        x3, _ = self.gcns[2](x)
    
        if self.gconv_activation:
            x3 = F.relu(x3)


        
        # after deformation 3
        x3 = self.gconv(x3)

        # sys.exit()

        if self.nn_decoder is not None:
            reconst = self.nn_decoder(img_feats)
        else:
            reconst = None

        return {
            "pred_coord": [x1, x2, x3],
            "pred_coord_before_deform": [init_pts, x1_up, x2_up],
            "reconst": reconst
        }




#TODO TEST SELF ATTENTION 
# class CliffordAttention(nn.Module):
#     def __init__(self, in_features, out_features, heads, algebra):
#         super(CliffordAttention, self).__init__()
#         self.in_features = in_features
#         self.out_features = out_features
#         self.heads = heads
#         self.algebra = algebra
#         self.scale = (in_features // heads) ** -0.5
        
#         self.to_queries = nn.Linear(in_features, out_features, bias=False)
#         self.to_keys = nn.Linear(in_features, out_features, bias=False)
#         self.to_values = nn.Linear(in_features, out_features, bias=False)
#         self.unify_heads = nn.Linear(out_features, in_features)

#     def forward(self, x):
#         b, n, _ = x.size()  # batch size, number of vertices, features
        
#         queries = self.to_queries(x).view(b, n, self.heads, self.out_features // self.heads)
#         keys = self.to_keys(x).view(b, n, self.heads, self.out_features // self.heads)
#         values = self.to_values(x).view(b, n, self.heads, self.out_features // self.heads)

#         # Calculate the geometric product as attention scores
#         # scores: (b, heads, n, n)
#         scores = self.algebra.geometric_product(queries, keys.transpose(-2, -1)) * self.scale
#         scores = F.softmax(scores, dim=-1)

#         # Attention-weighted sum of values
#         attended_values = torch.einsum('bhij,bhjd->bhid', scores, values)
#         attended_values = attended_values.contiguous().view(b, n, self.out_features)

#         return self.unify_heads(attended_values)






DOMANDE:
Originale:
Epoch 001, Step 000050/2408065, Time elapsed 0:00:21.297852, Loss 0.063401863 (0.146967320)
Epoch 001, Step 000100/2408065, Time elapsed 0:00:38.262325, Loss 0.040274613 (0.099145336)
Epoch 001, Step 000150/2408065, Time elapsed 0:00:55.270860, Loss 0.035927325 (0.078594115)
Epoch 001, Step 000200/2408065, Time elapsed 0:01:12.321128, Loss 0.029634126 (0.066952466)