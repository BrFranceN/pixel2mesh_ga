python entrypoint_predict.py --name nonso --options experiments/default/resnet.yml --checkpoint checkpoints/resnet.pth.tar --folder datasets/examples/


python entrypoint_predict.py --options /path/to/yml --checkpoint /path/to/checkpoint --folder /path/to/images






python entrypoint_train.py --name nonso --options experiments/default/resnet.yml






okay bene ora vorrei modificare questo codice per migliorarlo utilizzando la geometric algebra hai qualche idea da propormi? Che ne pensi modificare i livelli GCN in modo che siano E(3)-equivarianti, introducendo meccanismi di attenzione basati su GA. I GA consentono di gestire rotazioni e traslazioni in modo più naturale, quindi si potrebbero costruire livelli GCN che siano intrinsecamente equivarianti a tali trasformazioni, migliorando la capacità della rete di generalizzare tra diverse pose e scale dell'oggetto nell'immagine di input.
Implementare i livelli di attenzione utilizzando prodotti geometrici per calcolare i punteggi di attenzione tra i vertici della mesh, considerando sia le posizioni che le relazioni geometriche (come le direzioni dei bordi o le normali della superficie) rappresentate come multivettori.  Mi puoi aiutarlo a fare? Ho bisogno anche di capire se cio' funzionerebbe oppure no!
tuttavia sentiti libero di espormi le critiche di questo approccio e nel caso migliorarlo.

import torch
import torch.nn as nn
import torch.nn.functional as F

import sys # for stopping whenever I want!

from models.backbones import get_backbone
from models.layers.gbottleneck import GBottleneck
from models.layers.gconv import GConv
from models.layers.gpooling import GUnpooling
from models.layers.gprojection import GProjection


class P2MModel(nn.Module):

    def __init__(self, options, ellipsoid, camera_f, camera_c, mesh_pos):
        super(P2MModel, self).__init__()

        self.hidden_dim = options.hidden_dim
        self.coord_dim = options.coord_dim
        self.last_hidden_dim = options.last_hidden_dim
        self.init_pts = nn.Parameter(ellipsoid.coord, requires_grad=False)
        self.gconv_activation = options.gconv_activation

        self.nn_encoder, self.nn_decoder = get_backbone(options)
        self.features_dim = self.nn_encoder.features_dim + self.coord_dim

        self.gcns = nn.ModuleList([
            GBottleneck(6, self.features_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[0], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.coord_dim,
                        ellipsoid.adj_mat[1], activation=self.gconv_activation),
            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.last_hidden_dim,
                        ellipsoid.adj_mat[2], activation=self.gconv_activation)
        ])

        self.unpooling = nn.ModuleList([
            GUnpooling(ellipsoid.unpool_idx[0]),
            GUnpooling(ellipsoid.unpool_idx[1])
        ])

        # if options.align_with_tensorflow:
        #     self.projection = GProjection
        # else:
        #     self.projection = GProjection
        self.projection = GProjection(mesh_pos, camera_f, camera_c, bound=options.z_threshold,
                                      tensorflow_compatible=options.align_with_tensorflow)

        self.gconv = GConv(in_features=self.last_hidden_dim, out_features=self.coord_dim,
                           adj_mat=ellipsoid.adj_mat[2])

    def forward(self, img):
        batch_size = img.size(0)
        img_feats = self.nn_encoder(img)
        img_shape = self.projection.image_feature_shape(img)

        init_pts = self.init_pts.data.unsqueeze(0).expand(batch_size, -1, -1)


        # GCN Block 1
        x = self.projection(img_shape, img_feats, init_pts)
  
        x1, x_hidden = self.gcns[0](x)
        # x1 -> new coordinate of vertices => ex: torch.Size([8, 156, 3])
        # x_hidden -> feature learned during gcn => ex:  torch.Size([8, 156, 192])
    

        # before deformation 2
        x1_up = self.unpooling[0](x1)

        # GCN Block 2
        x = self.projection(img_shape, img_feats, x1)
       
        x = self.unpooling[0](torch.cat([x, x_hidden], 2))
  
        # after deformation 2
        x2, x_hidden = self.gcns[1](x)


        # before deformation 3
        x2_up = self.unpooling[1](x2)
 

        # GCN Block 3
        x = self.projection(img_shape, img_feats, x2)

        x = self.unpooling[1](torch.cat([x, x_hidden], 2))
   
        x3, _ = self.gcns[2](x)
    
        if self.gconv_activation:
            x3 = F.relu(x3)


        
        # after deformation 3
        x3 = self.gconv(x3)

        # sys.exit()

        if self.nn_decoder is not None:
            reconst = self.nn_decoder(img_feats)
        else:
            reconst = None

        return {
            "pred_coord": [x1, x2, x3],
            "pred_coord_before_deform": [init_pts, x1_up, x2_up],
            "reconst": reconst
        }




#TODO TEST SELF ATTENTION 
# class CliffordAttention(nn.Module):
#     def __init__(self, in_features, out_features, heads, algebra):
#         super(CliffordAttention, self).__init__()
#         self.in_features = in_features
#         self.out_features = out_features
#         self.heads = heads
#         self.algebra = algebra
#         self.scale = (in_features // heads) ** -0.5
        
#         self.to_queries = nn.Linear(in_features, out_features, bias=False)
#         self.to_keys = nn.Linear(in_features, out_features, bias=False)
#         self.to_values = nn.Linear(in_features, out_features, bias=False)
#         self.unify_heads = nn.Linear(out_features, in_features)

#     def forward(self, x):
#         b, n, _ = x.size()  # batch size, number of vertices, features
        
#         queries = self.to_queries(x).view(b, n, self.heads, self.out_features // self.heads)
#         keys = self.to_keys(x).view(b, n, self.heads, self.out_features // self.heads)
#         values = self.to_values(x).view(b, n, self.heads, self.out_features // self.heads)

#         # Calculate the geometric product as attention scores
#         # scores: (b, heads, n, n)
#         scores = self.algebra.geometric_product(queries, keys.transpose(-2, -1)) * self.scale
#         scores = F.softmax(scores, dim=-1)

#         # Attention-weighted sum of values
#         attended_values = torch.einsum('bhij,bhjd->bhid', scores, values)
#         attended_values = attended_values.contiguous().view(b, n, self.out_features)

#         return self.unify_heads(attended_values)






DOMANDE:
Originale:
Epoch 001, Step 000050/2408065, Time elapsed 0:00:21.297852, Loss 0.063401863 (0.146967320)
Epoch 001, Step 000100/2408065, Time elapsed 0:00:38.262325, Loss 0.040274613 (0.099145336)
Epoch 001, Step 000150/2408065, Time elapsed 0:00:55.270860, Loss 0.035927325 (0.078594115)
Epoch 001, Step 000200/2408065, Time elapsed 0:01:12.321128, Loss 0.029634126 (0.066952466)